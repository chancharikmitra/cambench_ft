[2025-04-06 09:27:50,010] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[INFO|2025-04-06 09:29:42] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:28162
W0406 09:29:43.849000 241828 site-packages/torch/distributed/run.py:792] 
W0406 09:29:43.849000 241828 site-packages/torch/distributed/run.py:792] *****************************************
W0406 09:29:43.849000 241828 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0406 09:29:43.849000 241828 site-packages/torch/distributed/run.py:792] *****************************************
[2025-04-06 09:29:56,333] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 09:29:56,333] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 09:29:56,333] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 09:29:56,333] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 09:29:56,333] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 09:29:56,334] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 09:29:56,394] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-04-06 09:29:57,922] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 09:29:57,922] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-06 09:29:57,922] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 09:29:57,922] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 09:29:57,926] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 09:29:57,935] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 09:29:57,939] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 09:29:57,968] [INFO] [comm.py:658:init_distributed] cdb=None
[WARNING|2025-04-06 09:29:59] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-04-06 09:29:59] llamafactory.hparams.parser:384 >> Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-04-06 09:29:59] llamafactory.hparams.parser:384 >> Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-04-06 09:29:59] llamafactory.hparams.parser:384 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-04-06 09:29:59] llamafactory.hparams.parser:384 >> Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-04-06 09:29:59] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-04-06 09:29:59] llamafactory.hparams.parser:384 >> Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-04-06 09:29:59] llamafactory.hparams.parser:384 >> Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[2025-04-06 09:30:03,611] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|configuration_utils.py:699] 2025-04-06 09:30:04,422 >> loading configuration file config.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/config.json
[INFO|configuration_utils.py:771] 2025-04-06 09:30:05,456 >> Model config Qwen2_5_VLConfig {
  "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "hidden_size": 1280,
    "in_chans": 3,
    "model_type": "qwen2_5_vl",
    "spatial_patch_size": 14,
    "tokens_per_second": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

Warning: The cache directory for DeepSpeed Triton autotune, /home/cmitra/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:32,101 >> loading file vocab.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/vocab.json
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:32,101 >> loading file merges.txt from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/merges.txt
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:32,101 >> loading file tokenizer.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/tokenizer.json
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:32,101 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:32,101 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:32,101 >> loading file tokenizer_config.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:32,101 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2313] 2025-04-06 09:31:32,391 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[INFO|image_processing_base.py:381] 2025-04-06 09:31:33,103 >> loading configuration file preprocessor_config.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/preprocessor_config.json
[INFO|image_processing_base.py:381] 2025-04-06 09:31:33,293 >> loading configuration file preprocessor_config.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/preprocessor_config.json
[WARNING|logging.py:329] 2025-04-06 09:31:33,293 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[INFO|image_processing_base.py:434] 2025-04-06 09:31:33,295 >> Image processor Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:34,020 >> loading file vocab.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/vocab.json
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:34,020 >> loading file merges.txt from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/merges.txt
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:34,020 >> loading file tokenizer.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/tokenizer.json
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:34,020 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:34,020 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:34,020 >> loading file tokenizer_config.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/5b5eecc7efc2c3e86839993f2689bbbdf06bd8d4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2050] 2025-04-06 09:31:34,020 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2313] 2025-04-06 09:31:34,244 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[rank6]:[W406 09:31:34.620824157 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[INFO|processing_utils.py:876] 2025-04-06 09:31:34,761 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)

{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-04-06 09:31:34] llamafactory.data.template:157 >> Add <|im_end|> to stop words.
[INFO|2025-04-06 09:31:34] llamafactory.data.loader:157 >> Loading dataset ./cam_motion/balanced_vqa.json...

Converting format of dataset (num_proc=16):   0%|          | 0/38672 [00:00<?, ? examples/s][rank5]:[W406 09:31:35.151898166 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.

Converting format of dataset (num_proc=16):   0%|          | 1/38672 [00:00<1:21:19,  7.92 examples/s]
Converting format of dataset (num_proc=16):   3%|▎         | 1236/38672 [00:00<00:08, 4505.80 examples/s][2025-04-06 09:31:35,419] [INFO] [comm.py:658:init_distributed] cdb=None
[rank7]:[W406 09:31:35.420356582 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.

Converting format of dataset (num_proc=16):   8%|▊         | 3119/38672 [00:00<00:04, 7589.12 examples/s]
Converting format of dataset (num_proc=16):  14%|█▍        | 5321/38672 [00:00<00:02, 11180.58 examples/s]
Converting format of dataset (num_proc=16):  28%|██▊       | 10762/38672 [00:00<00:01, 23222.95 examples/s][rank3]:[W406 09:31:35.809081630 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W406 09:31:35.809281438 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.

Converting format of dataset (num_proc=16):  35%|███▌      | 13651/38672 [00:01<00:02, 10246.85 examples/s][INFO|2025-04-06 09:31:36] llamafactory.hparams.parser:384 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16

Converting format of dataset (num_proc=16):  82%|████████▏ | 31665/38672 [00:01<00:00, 34555.78 examples/s]Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[rank2]:[W406 09:31:37.133535780 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W406 09:31:37.717719657 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.

Converting format of dataset (num_proc=16): 100%|██████████| 38672/38672 [00:03<00:00, 11765.18 examples/s]
[INFO|2025-04-06 09:31:38] llamafactory.data.loader:157 >> Loading dataset ./cam_motion/imbalanced_vqa.json...

Converting format of dataset (num_proc=16):   0%|          | 0/20440 [00:00<?, ? examples/s]
Converting format of dataset (num_proc=16):  19%|█▊        | 3788/20440 [00:00<00:00, 29575.68 examples/s]
Converting format of dataset (num_proc=16):  52%|█████▏    | 10696/20440 [00:00<00:00, 30789.25 examples/s]
Converting format of dataset (num_proc=16): 100%|██████████| 20440/20440 [00:00<00:00, 26698.46 examples/s]
[INFO|2025-04-06 09:31:39] llamafactory.data.loader:157 >> Loading dataset ./cam_motion/captionset.json...

Converting format of dataset (num_proc=16):   0%|          | 0/35050 [00:00<?, ? examples/s]
Converting format of dataset (num_proc=16):   0%|          | 1/35050 [00:00<1:25:25,  6.84 examples/s]
Converting format of dataset (num_proc=16):  30%|███       | 10517/35050 [00:00<00:00, 42066.96 examples/s]
Converting format of dataset (num_proc=16):  50%|█████     | 17529/35050 [00:00<00:00, 36888.54 examples/s]
Converting format of dataset (num_proc=16):  81%|████████  | 28398/35050 [00:00<00:00, 50628.63 examples/s]
Converting format of dataset (num_proc=16): 100%|██████████| 35050/35050 [00:00<00:00, 36335.99 examples/s]
[rank0]:[W406 09:31:41.608674964 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.

Running tokenizer on dataset (num_proc=16):   0%|          | 0/94162 [00:00<?, ? examples/s]
Running tokenizer on dataset (num_proc=16):   1%|          | 1000/94162 [31:14<48:29:56,  1.87s/ examples]
Running tokenizer on dataset (num_proc=16):   2%|▏         | 2000/94162 [31:17<19:48:22,  1.29 examples/s]
Running tokenizer on dataset (num_proc=16):   3%|▎         | 3000/94162 [31:22<10:42:19,  2.37 examples/s]
Running tokenizer on dataset (num_proc=16):   3%|▎         | 3000/94162 [31:40<10:42:19,  2.37 examples/s]
Running tokenizer on dataset (num_proc=16):   4%|▍         | 4000/94162 [32:02<6:48:10,  3.68 examples/s] 
Running tokenizer on dataset (num_proc=16):   5%|▌         | 5000/94162 [32:12<4:23:34,  5.64 examples/s]
Running tokenizer on dataset (num_proc=16):   5%|▌         | 5000/94162 [32:30<4:23:34,  5.64 examples/s]
Running tokenizer on dataset (num_proc=16):   6%|▋         | 6000/94162 [32:31<3:01:14,  8.11 examples/s]
Running tokenizer on dataset (num_proc=16):   7%|▋         | 7000/94162 [32:45<2:07:18, 11.41 examples/s]
Running tokenizer on dataset (num_proc=16):   7%|▋         | 7000/94162 [33:00<2:07:18, 11.41 examples/s]
Running tokenizer on dataset (num_proc=16):   8%|▊         | 8000/94162 [33:21<1:42:05, 14.07 examples/s]
Running tokenizer on dataset (num_proc=16):  10%|▉         | 9000/94162 [33:28<1:12:41, 19.53 examples/s]
Running tokenizer on dataset (num_proc=16):  10%|▉         | 9000/94162 [33:40<1:12:41, 19.53 examples/s]
Running tokenizer on dataset (num_proc=16):  11%|█         | 10000/94162 [33:45<57:09, 24.54 examples/s] 
Running tokenizer on dataset (num_proc=16):  11%|█         | 10000/94162 [34:00<57:09, 24.54 examples/s]
Running tokenizer on dataset (num_proc=16):  12%|█▏        | 11000/94162 [34:30<58:05, 23.86 examples/s]
Running tokenizer on dataset (num_proc=16):  12%|█▏        | 11000/94162 [34:40<58:05, 23.86 examples/s]
Running tokenizer on dataset (num_proc=16):  13%|█▎        | 12000/94162 [35:01<52:51, 25.91 examples/s]
Running tokenizer on dataset (num_proc=16):  13%|█▎        | 12000/94162 [35:20<52:51, 25.91 examples/s]
Running tokenizer on dataset (num_proc=16):  14%|█▍        | 13000/94162 [35:50<56:29, 23.95 examples/s]
Running tokenizer on dataset (num_proc=16):  14%|█▍        | 13000/94162 [36:00<56:29, 23.95 examples/s]
Running tokenizer on dataset (num_proc=16):  15%|█▍        | 14000/94162 [36:01<43:33, 30.67 examples/s]
Running tokenizer on dataset (num_proc=16):  16%|█▌        | 15000/94162 [36:20<43:00, 30.67 examples/s]
Running tokenizer on dataset (num_proc=16):  17%|█▋        | 16000/94162 [36:50<37:29, 34.75 examples/s]
Running tokenizer on dataset (num_proc=16):  17%|█▋        | 16000/94162 [37:00<37:29, 34.75 examples/s]
Running tokenizer on dataset (num_proc=16):  18%|█▊        | 17000/94162 [1:00:57<8:10:18,  2.62 examples/s]
Running tokenizer on dataset (num_proc=16):  19%|█▉        | 18000/94162 [1:01:57<6:17:03,  3.37 examples/s]
Running tokenizer on dataset (num_proc=16):  20%|██        | 19000/94162 [1:01:57<4:30:51,  4.62 examples/s]
Running tokenizer on dataset (num_proc=16):  20%|██        | 19000/94162 [1:02:10<4:30:51,  4.62 examples/s]
Running tokenizer on dataset (num_proc=16):  21%|██        | 20000/94162 [1:03:43<3:48:57,  5.40 examples/s]
Running tokenizer on dataset (num_proc=16):  22%|██▏       | 21000/94162 [1:04:49<3:04:33,  6.61 examples/s]
Running tokenizer on dataset (num_proc=16):  22%|██▏       | 21000/94162 [1:05:00<3:04:33,  6.61 examples/s]
Running tokenizer on dataset (num_proc=16):  23%|██▎       | 22000/94162 [1:05:16<2:18:32,  8.68 examples/s]
Running tokenizer on dataset (num_proc=16):  23%|██▎       | 22000/94162 [1:05:30<2:18:32,  8.68 examples/s]
Running tokenizer on dataset (num_proc=16):  24%|██▍       | 23000/94162 [1:06:07<1:54:30, 10.36 examples/s]
Running tokenizer on dataset (num_proc=16):  24%|██▍       | 23000/94162 [1:06:20<1:54:30, 10.36 examples/s]
Running tokenizer on dataset (num_proc=16):  25%|██▌       | 24000/94162 [1:07:19<1:44:11, 11.22 examples/s]
Running tokenizer on dataset (num_proc=16):  25%|██▌       | 24000/94162 [1:07:30<1:44:11, 11.22 examples/s]
Running tokenizer on dataset (num_proc=16):  27%|██▋       | 25000/94162 [1:07:32<1:16:50, 15.00 examples/s]
Running tokenizer on dataset (num_proc=16):  27%|██▋       | 25000/94162 [1:07:50<1:16:50, 15.00 examples/s]
Running tokenizer on dataset (num_proc=16):  28%|██▊       | 26000/94162 [1:08:12<1:06:47, 17.01 examples/s]
Running tokenizer on dataset (num_proc=16):  28%|██▊       | 26000/94162 [1:08:30<1:06:47, 17.01 examples/s]
Running tokenizer on dataset (num_proc=16):  29%|██▊       | 27000/94162 [1:09:31<1:12:28, 15.45 examples/s]
Running tokenizer on dataset (num_proc=16):  29%|██▊       | 27000/94162 [1:09:50<1:12:28, 15.45 examples/s]
Running tokenizer on dataset (num_proc=16):  30%|██▉       | 28000/94162 [1:11:00<1:19:15, 13.91 examples/s]
Running tokenizer on dataset (num_proc=16):  31%|███       | 29000/94162 [1:11:03<55:54, 19.42 examples/s]  
Running tokenizer on dataset (num_proc=16):  32%|███▏      | 30000/94162 [1:11:06<39:23, 27.15 examples/s]
Running tokenizer on dataset (num_proc=16):  32%|███▏      | 30000/94162 [1:11:20<39:23, 27.15 examples/s]
Running tokenizer on dataset (num_proc=16):  33%|███▎      | 31000/94162 [1:12:26<52:15, 20.14 examples/s]
Running tokenizer on dataset (num_proc=16):  33%|███▎      | 31000/94162 [1:12:40<52:15, 20.14 examples/s]
Running tokenizer on dataset (num_proc=16):  34%|███▍      | 32000/94162 [1:13:35<57:29, 18.02 examples/s]
Running tokenizer on dataset (num_proc=16):  34%|███▍      | 32000/94162 [1:13:50<57:29, 18.02 examples/s]
Running tokenizer on dataset (num_proc=16):  35%|███▌      | 33000/94162 [1:29:05<5:23:59,  3.15 examples/s]
Running tokenizer on dataset (num_proc=16):  36%|███▌      | 34000/94162 [1:31:38<4:29:05,  3.73 examples/s]
Running tokenizer on dataset (num_proc=16):  37%|███▋      | 35000/94162 [1:31:59<3:11:17,  5.15 examples/s]
Running tokenizer on dataset (num_proc=16):  37%|███▋      | 35000/94162 [1:32:10<3:11:17,  5.15 examples/s]
Running tokenizer on dataset (num_proc=16):  38%|███▊      | 36000/94162 [1:32:28<2:20:13,  6.91 examples/s]
Running tokenizer on dataset (num_proc=16):  38%|███▊      | 36000/94162 [1:32:40<2:20:13,  6.91 examples/s]
Running tokenizer on dataset (num_proc=16):  39%|███▉      | 37000/94162 [1:34:08<2:04:58,  7.62 examples/s]
Running tokenizer on dataset (num_proc=16):  39%|███▉      | 37000/94162 [1:34:20<2:04:58,  7.62 examples/s]
Running tokenizer on dataset (num_proc=16):  40%|████      | 38000/94162 [1:35:01<1:40:50,  9.28 examples/s]
Running tokenizer on dataset (num_proc=16):  41%|████▏     | 39000/94162 [1:35:09<1:11:42, 12.82 examples/s]
Running tokenizer on dataset (num_proc=16):  41%|████▏     | 39000/94162 [1:35:20<1:11:42, 12.82 examples/s]
Running tokenizer on dataset (num_proc=16):  42%|████▏     | 40000/94162 [1:35:27<54:05, 16.69 examples/s]  
Running tokenizer on dataset (num_proc=16):  42%|████▏     | 40000/94162 [1:35:40<54:05, 16.69 examples/s]
Running tokenizer on dataset (num_proc=16):  44%|████▎     | 41000/94162 [1:37:07<1:03:38, 13.92 examples/s]
Running tokenizer on dataset (num_proc=16):  44%|████▎     | 41000/94162 [1:37:20<1:03:38, 13.92 examples/s]
Running tokenizer on dataset (num_proc=16):  45%|████▍     | 42000/94162 [1:37:41<52:32, 16.55 examples/s]  
Running tokenizer on dataset (num_proc=16):  45%|████▍     | 42000/94162 [1:38:00<52:32, 16.55 examples/s]
Running tokenizer on dataset (num_proc=16):  46%|████▌     | 43000/94162 [1:39:35<1:05:22, 13.04 examples/s]
Running tokenizer on dataset (num_proc=16):  47%|████▋     | 44000/94162 [1:39:38<45:27, 18.39 examples/s]  
Running tokenizer on dataset (num_proc=16):  47%|████▋     | 44000/94162 [1:39:50<45:27, 18.39 examples/s]
Running tokenizer on dataset (num_proc=16):  48%|████▊     | 45000/94162 [1:40:42<46:54, 17.46 examples/s]
Running tokenizer on dataset (num_proc=16):  49%|████▉     | 46000/94162 [1:40:53<35:01, 22.92 examples/s]
Running tokenizer on dataset (num_proc=16):  49%|████▉     | 46000/94162 [1:41:10<35:01, 22.92 examples/s]
Running tokenizer on dataset (num_proc=16):  50%|████▉     | 47000/94162 [1:41:45<36:06, 21.77 examples/s]
Running tokenizer on dataset (num_proc=16):  50%|████▉     | 47000/94162 [1:42:00<36:06, 21.77 examples/s]
Running tokenizer on dataset (num_proc=16):  51%|█████     | 48000/94162 [1:42:52<40:21, 19.07 examples/s]
Running tokenizer on dataset (num_proc=16):  51%|█████     | 48000/94162 [1:43:10<40:21, 19.07 examples/s]
Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 49000/94162 [1:57:45<3:49:18,  3.28 examples/s]
Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 50000/94162 [1:59:28<2:59:42,  4.10 examples/s]
Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 51000/94162 [2:01:10<2:24:56,  4.96 examples/s]
Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 52000/94162 [2:01:50<1:47:26,  6.54 examples/s]
Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 52000/94162 [2:02:00<1:47:26,  6.54 examples/s]
Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 53000/94162 [2:02:15<1:18:38,  8.72 examples/s]
Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 53000/94162 [2:02:30<1:18:38,  8.72 examples/s]
Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 54000/94162 [2:03:03<1:03:17, 10.58 examples/s]
Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 54000/94162 [2:03:20<1:03:17, 10.58 examples/s]
Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 55000/94162 [2:04:04<55:07, 11.84 examples/s]  
Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 55000/94162 [2:04:20<55:07, 11.84 examples/s]
Running tokenizer on dataset (num_proc=16):  59%|█████▉    | 56000/94162 [2:05:38<55:32, 11.45 examples/s]
Running tokenizer on dataset (num_proc=16):  59%|█████▉    | 56000/94162 [2:05:50<55:32, 11.45 examples/s]
Running tokenizer on dataset (num_proc=16):  61%|██████    | 57000/94162 [2:06:18<45:17, 13.67 examples/s]
Running tokenizer on dataset (num_proc=16):  61%|██████    | 57000/94162 [2:06:30<45:17, 13.67 examples/s]
Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 58000/94162 [2:06:53<37:16, 16.17 examples/s]
Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 58000/94162 [2:07:10<37:16, 16.17 examples/s]
Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 59000/94162 [2:08:05<37:58, 15.43 examples/s]
Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 59000/94162 [2:08:20<37:58, 15.43 examples/s]
Running tokenizer on dataset (num_proc=16):  64%|██████▎   | 60000/94162 [2:11:42<1:02:49,  9.06 examples/s]
Running tokenizer on dataset (num_proc=16):  65%|██████▍   | 61000/94162 [2:12:07<46:58, 11.77 examples/s]  
Running tokenizer on dataset (num_proc=16):  65%|██████▍   | 61000/94162 [2:12:21<46:58, 11.77 examples/s]
Running tokenizer on dataset (num_proc=16):  66%|██████▌   | 62000/94162 [2:12:35<36:21, 14.74 examples/s]
Running tokenizer on dataset (num_proc=16):  66%|██████▌   | 62000/94162 [2:12:51<36:21, 14.74 examples/s]
Running tokenizer on dataset (num_proc=16):  67%|██████▋   | 63000/94162 [2:12:55<27:42, 18.74 examples/s]
Running tokenizer on dataset (num_proc=16):  67%|██████▋   | 63000/94162 [2:13:11<27:42, 18.74 examples/s]
Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 64000/94162 [2:15:11<39:17, 12.79 examples/s]
Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 65000/94162 [2:23:54<1:42:55,  4.72 examples/s]
Running tokenizer on dataset (num_proc=16):  70%|███████   | 66000/94162 [2:26:57<1:35:20,  4.92 examples/s]
Running tokenizer on dataset (num_proc=16):  71%|███████   | 67000/94162 [2:28:44<1:18:54,  5.74 examples/s]
Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 68000/94162 [2:28:54<54:26,  8.01 examples/s]  
Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 68000/94162 [2:29:11<54:26,  8.01 examples/s]
Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 69000/94162 [2:30:16<47:01,  8.92 examples/s]
Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 69000/94162 [2:30:31<47:01,  8.92 examples/s]
Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 70000/94162 [2:32:10<45:16,  8.89 examples/s]
Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 71000/94162 [2:32:34<33:11, 11.63 examples/s]
Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 71000/94162 [2:32:51<33:11, 11.63 examples/s]
Running tokenizer on dataset (num_proc=16):  76%|███████▋  | 72000/94162 [2:32:52<24:13, 15.25 examples/s]
Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 73000/94162 [2:33:03<17:24, 20.27 examples/s]
Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 73000/94162 [2:33:21<17:24, 20.27 examples/s]
Running tokenizer on dataset (num_proc=16):  79%|███████▊  | 74000/94162 [2:36:21<31:33, 10.65 examples/s]
Running tokenizer on dataset (num_proc=16):  80%|███████▉  | 75000/94162 [2:39:02<36:24,  8.77 examples/s]
Running tokenizer on dataset (num_proc=16):  81%|████████  | 76000/94162 [2:41:21<36:46,  8.23 examples/s]
Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 77000/94162 [2:42:23<29:37,  9.66 examples/s]
Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 77000/94162 [2:42:41<29:37,  9.66 examples/s]
Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 78000/94162 [2:42:55<22:07, 12.17 examples/s]
Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 78000/94162 [2:43:11<22:07, 12.17 examples/s]
Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 79000/94162 [2:43:26<16:54, 14.94 examples/s]
Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 79000/94162 [2:43:41<16:54, 14.94 examples/s]
Running tokenizer on dataset (num_proc=16):  85%|████████▍ | 80000/94162 [2:46:53<25:41,  9.19 examples/s]
Running tokenizer on dataset (num_proc=16):  86%|████████▌ | 80886/94162 [2:49:58<30:10,  7.33 examples/s]
Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 81771/94162 [2:51:21<25:40,  8.04 examples/s]
Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 81771/94162 [2:51:41<25:40,  8.04 examples/s]
Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 82656/94162 [2:53:18<24:14,  7.91 examples/s]
Running tokenizer on dataset (num_proc=16):  89%|████████▊ | 83541/94162 [2:55:05<22:09,  7.99 examples/s]
Running tokenizer on dataset (num_proc=16):  90%|████████▉ | 84426/94162 [2:55:07<14:25, 11.25 examples/s]
Running tokenizer on dataset (num_proc=16):  90%|████████▉ | 84426/94162 [2:55:21<14:25, 11.25 examples/s]
Running tokenizer on dataset (num_proc=16):  91%|█████████ | 85311/94162 [2:56:24<13:03, 11.30 examples/s]
Running tokenizer on dataset (num_proc=16):  91%|█████████ | 85311/94162 [2:56:41<13:03, 11.30 examples/s]
Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 86196/94162 [2:56:59<09:47, 13.55 examples/s]
Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 86196/94162 [2:57:11<09:47, 13.55 examples/s]
Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 87081/94162 [2:57:17<06:49, 17.30 examples/s]
Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 87967/94162 [2:57:19<04:16, 24.18 examples/s]
Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 87967/94162 [2:57:31<04:16, 24.18 examples/s]
Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 88852/94162 [3:01:50<10:40,  8.30 examples/s]
Running tokenizer on dataset (num_proc=16):  95%|█████████▌| 89737/94162 [3:03:22<08:30,  8.67 examples/s]
Running tokenizer on dataset (num_proc=16):  96%|█████████▌| 90622/94162 [3:06:31<08:32,  6.91 examples/s]
Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 91507/94162 [3:08:33<06:19,  7.00 examples/s]
Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 92392/94162 [3:09:15<03:22,  8.76 examples/s]
Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 92392/94162 [3:09:31<03:22,  8.76 examples/s]
Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 93277/94162 [3:10:04<01:25, 10.33 examples/s]
Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 93277/94162 [3:10:21<01:25, 10.33 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|██████████| 94162/94162 [3:14:16<00:00,  6.54 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|██████████| 94162/94162 [3:14:17<00:00,  8.08 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151653, 21468, 279, 6249, 3271, 2937, 745, 311, 279, 2115, 30, 5209, 1172, 4226, 7414, 476, 2308, 13, 151645, 198, 151644, 77091, 198, 9454, 151645, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|video_pad|><|vision_end|>Does the camera move laterally to the left? Please only answer Yes or No.<|im_end|>
<|im_start|>assistant
Yes<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9454, 151645, 198]
labels:
Yes<|im_end|>

[INFO|configuration_utils.py:699] 2025-04-06 12:46:26,415 >> loading configuration file config.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:771] 2025-04-06 12:46:26,417 >> Model config Qwen2_5_VLConfig {
  "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "hidden_size": 1280,
    "in_chans": 3,
    "model_type": "qwen2_5_vl",
    "spatial_patch_size": 14,
    "tokens_per_second": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}


Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s][INFO|modeling_utils.py:3982] 2025-04-06 12:46:27,416 >> loading weights file model.safetensors from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json

Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading shards:  40%|████      | 2/5 [00:00<00:00, 18.98it/s]
Downloading shards:  60%|██████    | 3/5 [00:00<00:00, 21.76it/s]
Downloading shards:  60%|██████    | 3/5 [00:00<00:00, 26.54it/s]
Downloading shards:  60%|██████    | 3/5 [00:00<00:00, 20.84it/s]
Downloading shards:  60%|██████    | 3/5 [00:00<00:00, 22.24it/s]
Downloading shards:  60%|██████    | 3/5 [00:00<00:00, 27.13it/s]
Downloading shards:  60%|██████    | 3/5 [00:00<00:00, 19.19it/s]
Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 25.45it/s]
[2025-04-06 12:46:27,631] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 30.99it/s]
[2025-04-06 12:46:27,634] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 21.56it/s]
[2025-04-06 12:46:27,636] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`

Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 21.60it/s]
[INFO|modeling_utils.py:4162] 2025-04-06 12:46:27,649 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-04-06 12:46:27,649] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8

Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 22.97it/s]
[2025-04-06 12:46:27,650] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[WARNING|logging.py:329] 2025-04-06 12:46:27,651 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
[WARNING|logging.py:329] 2025-04-06 12:46:27,651 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:1140] 2025-04-06 12:46:27,657 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:1633] 2025-04-06 12:46:27,658 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
[WARNING|logging.py:329] 2025-04-06 12:46:27,659 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`

Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 21.44it/s]
[2025-04-06 12:46:27,662] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`

Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 105.73it/s]
[2025-04-06 12:46:27,678] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`

Downloading shards: 100%|██████████| 5/5 [00:00<00:00,  6.13it/s]
Downloading shards: 100%|██████████| 5/5 [00:00<00:00,  6.67it/s]
[2025-04-06 12:46:28,167] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[2025-04-06 12:46:28,427] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 729, num_elems = 8.29B

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:06,  1.69s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:12,  3.23s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:06,  1.72s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:12,  3.25s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:13,  3.26s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:07,  1.82s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:07,  1.82s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:07,  1.84s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:04,  1.64s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:04<00:06,  2.28s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:04,  1.66s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:04<00:06,  2.29s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:04<00:06,  2.32s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:05,  1.68s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:05,  1.68s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:05,  1.68s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:06<00:04,  2.00s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:04<00:03,  1.66s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:06<00:04,  2.01s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:05<00:03,  1.67s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:06<00:04,  2.03s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:05<00:03,  1.68s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:05<00:03,  1.68s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:05<00:03,  1.68s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:06<00:01,  1.66s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:08<00:01,  1.88s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:08<00:01,  1.88s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:06<00:01,  1.67s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:06<00:01,  1.67s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:06<00:01,  1.68s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:06<00:01,  1.68s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:08<00:01,  1.92s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.41s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.76s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.31s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.47s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.47s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.79s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.34s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.49s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.32s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.49s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.33s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.49s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.33s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.50s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.66s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.91s/it]
[INFO|modeling_utils.py:4970] 2025-04-06 12:46:38,007 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.

[INFO|modeling_utils.py:4978] 2025-04-06 12:46:38,007 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1095] 2025-04-06 12:46:39,184 >> loading configuration file generation_config.json from cache at /home/cmitra/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:1140] 2025-04-06 12:46:39,184 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|2025-04-06 12:46:39] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-04-06 12:46:39] llamafactory.model.model_utils.attention:157 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-04-06 12:46:39] llamafactory.model.adapter:157 >> ZeRO3 / FSDP detected, remaining trainable params in float32.
[INFO|2025-04-06 12:46:39] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-04-06 12:46:39] llamafactory.model.model_utils.misc:157 >> Found linear modules: qkv,o_proj,gate_proj,k_proj,down_proj,v_proj,proj,q_proj,up_proj
[INFO|2025-04-06 12:46:45] llamafactory.model.loader:157 >> trainable params: 206,086,144 || all params: 8,498,252,800 || trainable%: 2.4250
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[INFO|trainer.py:746] 2025-04-06 12:46:52,391 >> Using auto half precision backend
[WARNING|trainer.py:781] 2025-04-06 12:46:52,392 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weight  torch.Size([64, 3420])base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weight
 Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.0.attn.proj.lora_a.default.weight  base_model.model.visual.blocks.2.attn.qkv.lora_b.default.weight torch.Size([64, 1280])torch.Size([3840, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weight  torch.Size([64, 1280])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3420])base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight
 torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.3.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3420])base_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight
 torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([1280, 64])base_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight
 torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.2.attn.qkv.lora_b.default.weight  torch.Size([1280, 64])torch.Size([3840, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64]) 
base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.4.attn.proj.lora_b.default.weight  torch.Size([64, 1280])torch.Size([1280, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.3.attn.qkv.lora_a.default.weight  torch.Size([3420, 64])
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weightWarning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.base_model.model.visual.blocks.3.attn.proj.lora_a.default.weight  
torch.Size([64, 3420])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.3.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weight torch.Size([3420, 64])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3420, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.5.attn.proj.lora_a.default.weight  base_model.model.visual.blocks.0.attn.qkv.lora_b.default.weighttorch.Size([64, 1280]) 
torch.Size([3840, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.5.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weight  torch.Size([1280, 64])torch.Size([64, 3420])

Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight  base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 1280]) base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.4.attn.qkv.lora_b.default.weight
  torch.Size([3420, 64])torch.Size([3840, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.4.attn.proj.lora_a.default.weight  torch.Size([64, 1280])torch.Size([64, 1280])Adam-mini found the param block with name:

 base_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.4.attn.proj.lora_b.default.weight  torch.Size([3420, 64])torch.Size([1280, 64])Adam-mini found the param block with name: 

base_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weight  base_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3420]) 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([64, 1280])base_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight 
base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weighttorch.Size([3420, 64]) 
torch.Size([1280, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.6.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3420, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.6.attn.proj.lora_a.default.weight  base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weighttorch.Size([64, 1280]) 
torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.6.attn.proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weight  torch.Size([1280, 64]) base_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weight
torch.Size([3840, 64]) 
torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.1.attn.proj.lora_a.default.weight  torch.Size([64, 1280])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.1.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weight   base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weighttorch.Size([1280, 64])torch.Size([3420, 64]) 

torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 1280])base_model.model.visual.blocks.5.attn.proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([64, 1280])base_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([3420, 64])base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: 
 torch.Size([3420, 64])base_model.model.visual.blocks.5.attn.proj.lora_b.default.weight
 torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weight  base_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3420])  
torch.Size([64, 1280])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3420, 64]) 
base_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.7.attn.qkv.lora_a.default.weight  torch.Size([64, 1280])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.2.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.7.attn.qkv.lora_b.default.weight  torch.Size([64, 1280])torch.Size([3840, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.2.attn.qkv.lora_b.default.weighttorch.Size([64, 3420]) 
torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.7.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weighttorch.Size([1280, 64]) 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3840, 64])base_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.6.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weight  torch.Size([64, 1280])torch.Size([3420, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.6.attn.proj.lora_b.default.weight  torch.Size([64, 1280])torch.Size([1280, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3420, 64]) 
base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3420, 64]) Adam-mini found the param block with name:base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weight  
base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3420]) 
torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weight base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weight base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weight torch.Size([1280, 64]) torch.Size([1280, 64])

torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weight  base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 3420])  
torch.Size([64, 1280])base_model.model.visual.blocks.3.attn.qkv.lora_b.default.weight
 torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64]) base_model.model.visual.blocks.8.attn.qkv.lora_b.default.weight 
torch.Size([3840, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.3.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([1280, 64]) base_model.model.visual.blocks.8.attn.proj.lora_a.default.weight
base_model.model.visual.blocks.7.attn.qkv.lora_a.default.weight  torch.Size([64, 1280])torch.Size([64, 1280])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.8.attn.proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.qkv.lora_b.default.weight torch.Size([1280, 64]) base_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight
torch.Size([3840, 64]) 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.7.attn.proj.lora_a.default.weight  torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([64, 1280])
 base_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight
 torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.7.attn.proj.lora_b.default.weighttorch.Size([3420, 64])base_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight
  torch.Size([1280, 64])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:  torch.Size([64, 3420])torch.Size([64, 1280])base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weight

 torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weight  torch.Size([1280, 64])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([1280, 64])base_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([64, 1280])base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight 
torch.Size([64, 1280])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.4.attn.qkv.lora_b.default.weight torch.Size([3420, 64])
torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.4.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.9.attn.qkv.lora_b.default.weight   torch.Size([64, 3420])torch.Size([3840, 64])
torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([1280, 64])base_model.model.visual.blocks.4.attn.proj.lora_b.default.weight
 torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.9.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 1280])base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weight
torch.Size([1280, 64]) 
torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.8.attn.proj.lora_a.default.weighttorch.Size([3420, 64]) Adam-mini found the param block with name:
torch.Size([64, 1280]) 
base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3420])base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weight  
torch.Size([64, 1280])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 1280]) base_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weight
base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weight  torch.Size([1280, 64])torch.Size([64, 1280])
Adam-mini found the param block with name:
 base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.10.attn.qkv.lora_a.default.weight   torch.Size([1280, 64])torch.Size([64, 1280])base_model.model.visual.blocks.5.attn.proj.lora_b.default.weight
 
torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name:
  base_model.model.visual.blocks.10.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([3420, 64])torch.Size([64, 1280]) 

base_model.model.visual.blocks.9.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weight  base_model.model.visual.blocks.9.attn.proj.lora_a.default.weight torch.Size([64, 1280])
torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weight  base_model.model.visual.blocks.9.attn.proj.lora_b.default.weighttorch.Size([3420, 64])torch.Size([64, 1280])
 
torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280]) 
torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3840, 64])base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weight 
 base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weight torch.Size([3420, 64])torch.Size([64, 3420])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weight  base_model.model.visual.blocks.6.attn.proj.lora_a.default.weight torch.Size([1280, 64])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weighttorch.Size([64, 1280]) 
Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.10.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280]) Adam-mini found the param block with name:
torch.Size([3420, 64]) 
base_model.model.visual.blocks.11.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64]) 
base_model.model.visual.blocks.10.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.10.attn.proj.lora_a.default.weight  base_model.model.visual.blocks.11.attn.proj.lora_a.default.weight torch.Size([3420, 64]) torch.Size([64, 1280])

torch.Size([64, 1280])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.10.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.11.attn.proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64])torch.Size([1280, 64])
 
base_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3420, 64]) Adam-mini found the param block with name:
base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.7.attn.qkv.lora_a.default.weight torch.Size([3420, 64])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.7.attn.qkv.lora_b.default.weight  torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([3840, 64])

 base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.7.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weight torch.Size([1280, 64])Adam-mini found the param block with name:
 torch.Size([64, 3420])base_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weight
 torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weighttorch.Size([1280, 64]) Adam-mini found the param block with name:
torch.Size([1280, 64]) 
base_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.Adam-mini found the param block with name:   
base_model.model.visual.blocks.12.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.11.attn.qkv.lora_b.default.weight   torch.Size([64, 1280])torch.Size([3840, 64])torch.Size([64, 1280])


Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3420, 64])base_model.model.visual.blocks.12.attn.qkv.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([3840, 64])base_model.model.visual.blocks.11.attn.proj.lora_a.default.weight
 torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([1280, 64])base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weight
 torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.12.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weight  torch.Size([1280, 64])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weighttorch.Size([3420, 64]) torch.Size([64, 1280])
 base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weight
 Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_b.default.weighttorch.Size([64, 1280]) 
torch.Size([3840, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weight 
 torch.Size([64, 1280])
Adam-mini found the param block with name:torch.Size([3420, 64]) 
base_model.model.visual.blocks.8.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.0.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.8.attn.proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([3420, 64])  
base_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weighttorch.Size([1280, 64])torch.Size([3840, 64])
 
torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([3420, 64])torch.Size([64, 3420]) 
base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([64, 1280]) Adam-mini found the param block with name:
base_model.model.visual.blocks.0.attn.proj.lora_a.default.weight  base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])   
base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weighttorch.Size([1280, 64])  
torch.Size([64, 3420])Adam-mini found the param block with name:torch.Size([3420, 64])
 
base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.12.attn.qkv.lora_a.default.weighttorch.Size([64, 1280]) 
Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])  
 base_model.model.visual.blocks.13.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.12.attn.qkv.lora_b.default.weight  torch.Size([3420, 64])torch.Size([3840, 64])
torch.Size([64, 1280])
Adam-mini found the param block with name:
 base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.12.attn.proj.lora_a.default.weight  torch.Size([64, 3420])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.12.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.13.attn.proj.lora_a.default.weight  torch.Size([64, 1280]) torch.Size([1280, 64])base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weighttorch.Size([64, 1280])

 
torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.13.attn.proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([3420, 64]) Adam-mini found the param block with name:
base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weighttorch.Size([1280, 64])  
base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weighttorch.Size([64, 1280]) 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([3420, 64])base_model.model.visual.blocks.9.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weight 
  torch.Size([3840, 64])torch.Size([64, 3420])
base_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weight base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.9.attn.proj.lora_a.default.weighttorch.Size([1280, 64]) torch.Size([3420, 64]) 
torch.Size([64, 1280])
torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.9.attn.proj.lora_b.default.weight Adam-mini found the param block with name:  torch.Size([3420, 64])base_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weighttorch.Size([1280, 64])
 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) Adam-mini found the param block with name:base_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weight
  base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([3420, 64]) Adam-mini found the param block with name:torch.Size([64, 3420])
base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight 
 base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weighttorch.Size([64, 1280]) Adam-mini found the param block with name:
 torch.Size([3840, 64])base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([1280, 64])base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weight
 Adam-mini found the param block with name:torch.Size([3420, 64]) 
base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.13.attn.qkv.lora_a.default.weighttorch.Size([1280, 64])   
base_model.model.visual.blocks.1.attn.proj.lora_b.default.weighttorch.Size([64, 1280]) 
torch.Size([64, 1280])torch.Size([1280, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3420, 64]) 
base_model.model.visual.blocks.13.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weight base_model.model.visual.blocks.14.attn.qkv.lora_a.default.weight  base_model.model.visual.blocks.13.attn.proj.lora_a.default.weight base_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 1280]) torch.Size([64, 1280])torch.Size([64, 1280])

torch.Size([64, 3420])

Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([1280, 64]) Adam-mini found the param block with name: 
 base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weight base_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight torch.Size([3840, 64]) torch.Size([1280, 64])

torch.Size([3420, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.14.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.10.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name: torch.Size([1280, 64])torch.Size([64, 1280])torch.Size([64, 1280]) 

base_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.10.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weight 
 torch.Size([3420, 64])torch.Size([64, 1280])Adam-mini found the param block with name:

 base_model.model.visual.blocks.10.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.visual.blocks.10.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([1280, 64])torch.Size([64, 3420])base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: 

 torch.Size([64, 3420])base_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weight
 torch.Size([64, 1280])Adam-mini found the param block with name:
Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:    base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weighttorch.Size([1280, 64])torch.Size([1280, 64])base_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weight

  torch.Size([64, 1280])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.14.attn.qkv.lora_a.default.weight  base_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([64, 3420]) 
Adam-mini found the param block with name:
base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weight  base_model.model.visual.blocks.2.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([64, 1280])  
torch.Size([64, 1280])base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weight
  Adam-mini found the param block with name:torch.Size([1280, 64])torch.Size([3840, 64]) 
base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weight
Adam-mini found the param block with name:  torch.Size([3420, 64])base_model.model.visual.blocks.2.attn.qkv.lora_b.default.weight
 torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.14.attn.proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weight base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weight torch.Size([1280, 64]) torch.Size([64, 3420])
 base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight
torch.Size([64, 1280]) 
torch.Size([64, 1280])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weightAdam-mini found the param block with name: torch.Size([1280, 64]) torch.Size([3840, 64]) 

base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.2.attn.proj.lora_b.default.weight  torch.Size([64, 1280])
torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.visual.blocks.15.attn.proj.lora_a.default.weight  base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight  base_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.11.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.15.attn.proj.lora_b.default.weight    torch.Size([64, 1280])torch.Size([64, 1280])torch.Size([1280, 64])torch.Size([3840, 64])



Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3420, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.11.attn.proj.lora_a.default.weight  torch.Size([64, 1280]) 
torch.Size([3420, 64])base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:
  Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.11.attn.proj.lora_b.default.weight
Adam-mini found the param block with name:   torch.Size([64, 3420])torch.Size([1280, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weight

  base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weighttorch.Size([3420, 64])Adam-mini found the param block with name:
  torch.Size([64, 1280])base_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:
  base_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weighttorch.Size([1280, 64]) 
torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight torch.Size([64, 1280])base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight  
Adam-mini found the param block with name:torch.Size([3420, 64])torch.Size([3420, 64]) 

Adam-mini found the param block with name:base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weight  base_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weighttorch.Size([64, 1280]) 
torch.Size([3420, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weight  base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weighttorch.Size([3840, 64]) Adam-mini found the param block with name:
torch.Size([64, 1280]) Adam-mini found the param block with name:
base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weight  base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([64, 3420]) torch.Size([64, 3420])Adam-mini found the param block with name: 
base_model.model.visual.blocks.15.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weight
  torch.Size([3420, 64])torch.Size([64, 1280])Adam-mini found the param block with name:
Adam-mini found the param block with name: 
 base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([1280, 64])torch.Size([1280, 64])base_model.model.visual.blocks.15.attn.proj.lora_b.default.weightAdam-mini found the param block with name:

  base_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weighttorch.Size([1280, 64]) 
torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])  base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])

  base_model.model.visual.blocks.12.attn.qkv.lora_a.default.weight torch.Size([3420, 64])torch.Size([64, 1280])
Adam-mini found the param block with name:
Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.16.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.3.attn.qkv.lora_b.default.weight   base_model.model.visual.blocks.12.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:torch.Size([3840, 64])torch.Size([3840, 64])  
base_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weight
torch.Size([3840, 64]) 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.3.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.12.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.16.attn.proj.lora_a.default.weight Adam-mini found the param block with name:  torch.Size([64, 1280]) torch.Size([64, 1280])
torch.Size([64, 1280])
base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weight
 torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.16.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.3.attn.proj.lora_b.default.weight   torch.Size([1280, 64])Adam-mini found the param block with name:torch.Size([1280, 64])
torch.Size([1280, 64])
 
base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weight    torch.Size([64, 1280])torch.Size([3420, 64])torch.Size([64, 1280])torch.Size([64, 1280])



Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.16.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:  torch.Size([3420, 64]) torch.Size([3840, 64])torch.Size([3420, 64])
base_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weight

 torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight  base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.16.attn.proj.lora_b.default.weight base_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weight  torch.Size([64, 1280]) torch.Size([1280, 64])torch.Size([64, 3420])

torch.Size([1280, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight  torch.Size([1280, 64])torch.Size([3420, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:
  base_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.13.attn.qkv.lora_a.default.weight  torch.Size([64, 1280])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.13.attn.qkv.lora_b.default.weight    torch.Size([3420, 64])base_model.model.visual.blocks.17.attn.qkv.lora_a.default.weighttorch.Size([64, 3420])torch.Size([3840, 64])
 

torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weight  base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weight base_model.model.visual.blocks.13.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weight torch.Size([1280, 64])  torch.Size([3840, 64])
torch.Size([64, 1280])
torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.13.attn.proj.lora_b.default.weight  torch.Size([3420, 64])torch.Size([1280, 64])Adam-mini found the param block with name:

 base_model.model.visual.blocks.17.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.visual.blocks.17.attn.proj.lora_b.default.weight  torch.Size([64, 3420]) torch.Size([64, 1280])
torch.Size([1280, 64])base_model.model.visual.blocks.4.attn.qkv.lora_b.default.weight

 Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name: 
base_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weight  base_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weighttorch.Size([1280, 64]) 
torch.Size([3420, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.4.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([3420, 64])  
base_model.model.visual.blocks.17.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weight
  torch.Size([64, 1280])torch.Size([64, 1280])Adam-mini found the param block with name:

 base_model.model.visual.blocks.4.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weight  torch.Size([3840, 64]) Adam-mini found the param block with name:torch.Size([64, 1280])
torch.Size([3420, 64])
 
base_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.17.attn.proj.lora_a.default.weighttorch.Size([3420, 64]) Adam-mini found the param block with name:
Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight
  base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weighttorch.Size([3420, 64]) Adam-mini found the param block with name:
 torch.Size([64, 3420])Adam-mini found the param block with name:base_model.model.visual.blocks.17.attn.proj.lora_b.default.weight
  base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weighttorch.Size([1280, 64]) 
Adam-mini found the param block with name:torch.Size([64, 3420]) 
base_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name:  base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weight
base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([64, 1280])base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weighttorch.Size([1280, 64])
 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([3420, 64])base_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.14.attn.qkv.lora_a.default.weight
  torch.Size([64, 1280])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: 
  base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:    torch.Size([3840, 64])base_model.model.visual.blocks.14.attn.proj.lora_a.default.weight
torch.Size([64, 3420])torch.Size([64, 1280]) 

torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.14.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weight    base_model.model.visual.blocks.18.attn.proj.lora_a.default.weighttorch.Size([1280, 64])torch.Size([3420, 64])torch.Size([1280, 64]) 


torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weight  base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3420]) Adam-mini found the param block with name:
Adam-mini found the param block with name:torch.Size([64, 1280]) Adam-mini found the param block with name:
 base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weight    Adam-mini found the param block with name:base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])  
torch.Size([1280, 64])torch.Size([64, 1280])base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight

Adam-mini found the param block with name: torch.Size([64, 1280]) Adam-mini found the param block with name:torch.Size([3420, 64])base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weight

  base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weight torch.Size([3840, 64])torch.Size([3420, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.0.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weight   torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3840, 64])torch.Size([64, 1280]) 

base_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weight 
 base_model.model.visual.blocks.5.attn.proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])  base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weight
 torch.Size([64, 1280]) base_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weight
 Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name:torch.Size([3420, 64]) 
Adam-mini found the param block with name:  
base_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.0.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.5.attn.proj.lora_b.default.weight   torch.Size([3420, 64])torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([1280, 64])

Adam-mini found the param block with name:  base_model.model.visual.blocks.18.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weight
  Adam-mini found the param block with name:torch.Size([64, 1280]) torch.Size([64, 3420])
base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight
Adam-mini found the param block with name:  torch.Size([1280, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_b.default.weight  torch.Size([64, 1280]) base_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weight
torch.Size([1280, 64])  
Adam-mini found the param block with name:torch.Size([1280, 64])torch.Size([64, 3420])Adam-mini found the param block with name:  

base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight  torch.Size([64, 1280])torch.Size([3420, 64])
Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([1280, 64]) 

base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([3420, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weight   base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280]) torch.Size([3420, 64])
torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:
 
Adam-mini found the param block with name:  base_model.model.visual.blocks.19.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weight   base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weight torch.Size([64, 1280])torch.Size([64, 1280]) 
torch.Size([3840, 64])Adam-mini found the param block with name:torch.Size([3420, 64])

 
base_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 1280])base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight
Adam-mini found the param block with name: torch.Size([3420, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:  torch.Size([3840, 64])
 base_model.model.visual.blocks.15.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weight
   torch.Size([64, 1280])torch.Size([3420, 64])
torch.Size([64, 3420])

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.15.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:   base_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.19.attn.proj.lora_a.default.weight torch.Size([64, 3420]) torch.Size([1280, 64]) torch.Size([64, 3420])

torch.Size([1280, 64])torch.Size([64, 1280])


Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name: 
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weight base_model.model.visual.blocks.19.attn.proj.lora_b.default.weight base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weighttorch.Size([1280, 64]) 
 torch.Size([1280, 64])
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 1280])base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weight
 torch.Size([3420, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  
  base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.6.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.19.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight    Adam-mini found the param block with name:torch.Size([64, 1280]) torch.Size([3840, 64])torch.Size([64, 1280])torch.Size([64, 1280])

base_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weight

 Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight
 Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3840, 64]) base_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weight
torch.Size([3840, 64]) base_model.model.visual.blocks.6.attn.proj.lora_a.default.weight  
torch.Size([3420, 64])torch.Size([3420, 64])torch.Size([64, 1280])


Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name: 
 base_model.model.visual.blocks.19.attn.proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weight   base_model.model.visual.blocks.1.attn.proj.lora_a.default.weighttorch.Size([1280, 64])torch.Size([64, 3420]) base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight
 
Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([64, 1280])
 Adam-mini found the param block with name:
base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weight  torch.Size([64, 1280])base_model.model.visual.blocks.1.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weighttorch.Size([1280, 64])base_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight 
 torch.Size([1280, 64])
torch.Size([3420, 64])torch.Size([64, 1280])


Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weight base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weight  base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weight   torch.Size([64, 1280])
torch.Size([64, 1280])

torch.Size([64, 1280])torch.Size([1280, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weighttorch.Size([3840, 64])  torch.Size([3420, 64])
torch.Size([3420, 64])torch.Size([3420, 64])


Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.16.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.20.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:     torch.Size([64, 1280])base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weighttorch.Size([64, 1280])base_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight 
 
 torch.Size([64, 3420])torch.Size([64, 3420])Adam-mini found the param block with name:torch.Size([64, 1280]) 

base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:
  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.16.attn.proj.lora_b.default.weighttorch.Size([3840, 64])   Adam-mini found the param block with name:base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weight
base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])  base_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weight
torch.Size([1280, 64])torch.Size([1280, 64]) 

torch.Size([3420, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.20.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([3420, 64])  Adam-mini found the param block with name:
base_model.model.visual.blocks.7.attn.qkv.lora_a.default.weighttorch.Size([64, 3420]) Adam-mini found the param block with name:base_model.model.visual.blocks.20.attn.qkv.lora_a.default.weight 
 torch.Size([64, 1280]) base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:
 Adam-mini found the param block with name: torch.Size([64, 1280])torch.Size([64, 1280]) base_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:

base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weight   Adam-mini found the param block with name:torch.Size([64, 1280])base_model.model.visual.blocks.7.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weight  
 Adam-mini found the param block with name:torch.Size([3840, 64])base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weighttorch.Size([3840, 64]) 

base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weight  torch.Size([3420, 64])torch.Size([3420, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.7.attn.proj.lora_a.default.weight
    base_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.2.attn.qkv.lora_a.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name:    
torch.Size([64, 1280])torch.Size([64, 3420])torch.Size([64, 1280])base_model.model.visual.blocks.20.attn.proj.lora_b.default.weight


Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  torch.Size([1280, 64])  base_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.2.attn.qkv.lora_b.default.weight
base_model.model.visual.blocks.7.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weight   torch.Size([1280, 64]) torch.Size([3420, 64])
torch.Size([3840, 64])torch.Size([1280, 64])
Adam-mini found the param block with name:

 base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3420, 64])base_model.model.visual.blocks.17.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:
 Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weight torch.Size([64, 1280])base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight base_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([64, 3420]) Adam-mini found the param block with name:torch.Size([64, 1280])  
torch.Size([64, 1280])base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weight
Adam-mini found the param block with name:
   torch.Size([3840, 64])torch.Size([64, 1280])Adam-mini found the param block with name:
base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name: 
base_model.model.visual.blocks.2.attn.proj.lora_b.default.weight  torch.Size([1280, 64])base_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:
torch.Size([1280, 64]) 
  torch.Size([3420, 64])base_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weight
 base_model.model.visual.blocks.17.attn.proj.lora_a.default.weight torch.Size([3420, 64])
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([1280, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight
   base_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weight base_model.model.visual.blocks.21.attn.qkv.lora_a.default.weighttorch.Size([64, 1280]) base_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weight
torch.Size([64, 1280])  Adam-mini found the param block with name:torch.Size([64, 1280])
torch.Size([64, 3420])Adam-mini found the param block with name:
 
 Adam-mini found the param block with name:base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([3420, 64])torch.Size([64, 1280])base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weight 
base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weight
   torch.Size([3840, 64])torch.Size([1280, 64])Adam-mini found the param block with name:torch.Size([3420, 64])

 
base_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3420, 64])base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weight
 torch.Size([64, 1280])Adam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_a.default.weight base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.21.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 1280])base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight  torch.Size([64, 1280])torch.Size([64, 3420])
 base_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weight

torch.Size([3420, 64]) Adam-mini found the param block with name:
Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name: 
 base_model.model.visual.blocks.21.attn.proj.lora_b.default.weight base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weight base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name: 
 
 base_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weighttorch.Size([1280, 64])base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weight
  torch.Size([3420, 64])torch.Size([64, 3420])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.21.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])  
base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])  Adam-mini found the param block with name:
Adam-mini found the param block with name:base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weighttorch.Size([1280, 64])   Adam-mini found the param block with name:
base_model.model.visual.blocks.21.attn.proj.lora_b.default.weight base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weighttorch.Size([64, 3420]) base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weight 
torch.Size([1280, 64]) torch.Size([64, 1280])
torch.Size([3420, 64])
Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weight  base_model.model.visual.blocks.8.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:torch.Size([1280, 64]) Adam-mini found the param block with name: 
torch.Size([3840, 64]) Adam-mini found the param block with name:base_model.model.visual.blocks.3.attn.qkv.lora_a.default.weight
base_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight   base_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weighttorch.Size([64, 1280]) torch.Size([64, 1280])

torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weight base_model.model.visual.blocks.3.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:   base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.8.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weighttorch.Size([3840, 64])torch.Size([64, 1280])   

torch.Size([3420, 64])torch.Size([64, 1280])
torch.Size([3420, 64])
Adam-mini found the param block with name:
 base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weight torch.Size([3840, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.8.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.3.attn.proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weight torch.Size([1280, 64])Adam-mini found the param block with name:  
 torch.Size([64, 1280])base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weighttorch.Size([64, 1280])base_model.model.visual.blocks.18.attn.proj.lora_a.default.weight

  torch.Size([64, 3420])torch.Size([64, 1280])Adam-mini found the param block with name:

 base_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:
   base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.18.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([1280, 64])torch.Size([64, 1280])  

Adam-mini found the param block with name:torch.Size([1280, 64])base_model.model.visual.blocks.3.attn.proj.lora_b.default.weight 
 Adam-mini found the param block with name:base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weighttorch.Size([1280, 64]) 
 base_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 3420]) 
base_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weighttorch.Size([3420, 64]) 
Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weight
base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight   Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([1280, 64])Adam-mini found the param block with name:
torch.Size([64, 1280]) 
 
base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weightAdam-mini found the param block with name: torch.Size([3420, 64])  torch.Size([64, 1280])
base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weighttorch.Size([3840, 64])
Adam-mini found the param block with name:
  base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weighttorch.Size([3420, 64])Adam-mini found the param block with name: 
 torch.Size([64, 1280])base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:
 Adam-mini found the param block with name: torch.Size([3420, 64]) base_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.22.attn.proj.lora_a.default.weightAdam-mini found the param block with name: 
Adam-mini found the param block with name:   torch.Size([64, 1280])torch.Size([64, 1280])base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight

  torch.Size([3840, 64])Adam-mini found the param block with name:torch.Size([64, 1280])
Adam-mini found the param block with name:
 Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.22.attn.proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weight  Adam-mini found the param block with name:  torch.Size([3420, 64])base_model.model.visual.blocks.22.attn.proj.lora_a.default.weighttorch.Size([1280, 64])base_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight
torch.Size([64, 3420]) 
 
torch.Size([3420, 64])torch.Size([64, 1280])
Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weighttorch.Size([1280, 64])  Adam-mini found the param block with name:base_model.model.visual.blocks.22.attn.proj.lora_b.default.weighttorch.Size([64, 3420]) Adam-mini found the param block with name:

base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weight  torch.Size([1280, 64]) base_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])
  torch.Size([64, 3420])
base_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weight
 torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weighttorch.Size([3420, 64])  
base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weight base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weight torch.Size([1280, 64]) torch.Size([64, 1280])
torch.Size([64, 1280])Adam-mini found the param block with name:

 base_model.model.visual.blocks.19.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weight  base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.visual.blocks.9.attn.qkv.lora_b.default.weight
 Adam-mini found the param block with name:base_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight torch.Size([3420, 64]) Adam-mini found the param block with name:  torch.Size([3840, 64])base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weight
base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight
 torch.Size([3840, 64]) torch.Size([64, 1280])torch.Size([3420, 64])


Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name: 
 Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.4.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.9.attn.proj.lora_a.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weight base_model.model.visual.blocks.19.attn.proj.lora_a.default.weight  torch.Size([3840, 64]) torch.Size([64, 3420]) torch.Size([64, 1280])base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weight
torch.Size([64, 1280])

 
torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.19.attn.proj.lora_b.default.weighttorch.Size([1280, 64])Adam-mini found the param block with name: 
base_model.model.visual.blocks.9.attn.proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([1280, 64])  base_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weight
torch.Size([1280, 64])base_model.model.visual.blocks.4.attn.proj.lora_a.default.weight 
 torch.Size([64, 3420])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:    base_model.model.visual.blocks.23.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.4.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight   torch.Size([1280, 64])Adam-mini found the param block with name: torch.Size([64, 1280])torch.Size([64, 1280])
 
torch.Size([1280, 64])
base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])  
base_model.model.visual.blocks.23.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name:torch.Size([3420, 64])  

base_model.model.visual.blocks.23.attn.qkv.lora_a.default.weight base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weight   torch.Size([64, 1280])torch.Size([64, 1280])torch.Size([3420, 64])


Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name:  
 base_model.model.visual.blocks.23.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.23.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])   base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weight 
torch.Size([3840, 64])base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weighttorch.Size([3420, 64]) Adam-mini found the param block with name:

 torch.Size([64, 1280]) torch.Size([3420, 64])base_model.model.visual.blocks.23.attn.proj.lora_b.default.weight

 torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 1280])base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name: 
base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([3420, 64]) base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
  torch.Size([64, 3420])
torch.Size([64, 1280])base_model.model.visual.blocks.23.attn.proj.lora_b.default.weight

 Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name:Adam-mini found the param block with name: 
Adam-mini found the param block with name:base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weight    base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weighttorch.Size([3420, 64])base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weight   
torch.Size([1280, 64])torch.Size([64, 3420])Adam-mini found the param block with name:torch.Size([3420, 64]) 
base_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weight

 torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name:
  base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([3420, 64])torch.Size([64, 3420])base_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weight

 torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:   base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weight base_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.10.attn.qkv.lora_a.default.weight  base_model.model.visual.blocks.20.attn.qkv.lora_a.default.weight  torch.Size([1280, 64])torch.Size([3420, 64])torch.Size([64, 1280])torch.Size([64, 1280])torch.Size([64, 1280])




Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([3420, 64])base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weight 
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_b.default.weight torch.Size([3840, 64]) Adam-mini found the param block with name:base_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weight
 torch.Size([3840, 64]) 
base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 3420])  torch.Size([64, 1280])base_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weight
Adam-mini found the param block with name:
  base_model.model.visual.blocks.20.attn.proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.10.attn.proj.lora_a.default.weight
  base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weight   torch.Size([1280, 64])torch.Size([64, 1280])torch.Size([3840, 64])Adam-mini found the param block with name:torch.Size([1280, 64])



 base_model.model.visual.blocks.20.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.5.attn.proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:  base_model.model.visual.blocks.24.attn.qkv.lora_a.default.weight
 base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.24.attn.qkv.lora_a.default.weighttorch.Size([64, 1280])  Adam-mini found the param block with name:
  torch.Size([64, 1280])torch.Size([64, 1280])base_model.model.visual.blocks.5.attn.proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])

  
base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weighttorch.Size([1280, 64])Adam-mini found the param block with name: Adam-mini found the param block with name:
Adam-mini found the param block with name:torch.Size([3420, 64])  
 base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.24.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.24.attn.qkv.lora_b.default.weight   torch.Size([3420, 64])torch.Size([3840, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:torch.Size([3840, 64])
  base_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weight 
 torch.Size([64, 1280])torch.Size([64, 1280])
Adam-mini found the param block with name:
 base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.proj.lora_a.default.weighttorch.Size([64, 1280])  base_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weight 
torch.Size([64, 1280])base_model.model.visual.blocks.24.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:
  torch.Size([3420, 64]) torch.Size([64, 1280])torch.Size([3420, 64])Adam-mini found the param block with name:
base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weight

  base_model.model.visual.blocks.24.attn.proj.lora_b.default.weighttorch.Size([3420, 64]) Adam-mini found the param block with name:
torch.Size([1280, 64]) 
base_model.model.visual.blocks.24.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([1280, 64])base_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: 
 Adam-mini found the param block with name:torch.Size([64, 3420])Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weight
  base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weight
  torch.Size([64, 3420]) torch.Size([64, 1280])
torch.Size([1280, 64])base_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:
 
 Adam-mini found the param block with name:base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name:   
torch.Size([3420, 64])base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weight
 Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([1280, 64]) torch.Size([3420, 64]) base_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.21.attn.qkv.lora_a.default.weight
 
 Adam-mini found the param block with name:torch.Size([64, 1280]) 
torch.Size([3420, 64])base_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([64, 3420])base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:
  base_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weighttorch.Size([3840, 64]) Adam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weighttorch.Size([64, 1280]) base_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weight 
base_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weight torch.Size([64, 1280])Adam-mini found the param block with name: torch.Size([64, 1280])Adam-mini found the param block with name:
torch.Size([1280, 64]) 
 
base_model.model.visual.blocks.21.attn.proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weight  torch.Size([64, 1280]) base_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.11.attn.qkv.lora_b.default.weighttorch.Size([3420, 64]) 

 torch.Size([3420, 64])
torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weight     torch.Size([64, 3420])base_model.model.visual.blocks.6.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.11.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight
torch.Size([64, 3420])   torch.Size([3840, 64])
torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([64, 1280])

 
Adam-mini found the param block with name:base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64]) Adam-mini found the param block with name:base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weight
torch.Size([1280, 64])  
Adam-mini found the param block with name:base_model.model.visual.blocks.11.attn.proj.lora_b.default.weighttorch.Size([3420, 64])  
base_model.model.visual.blocks.6.attn.proj.lora_a.default.weighttorch.Size([1280, 64]) 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([1280, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.visual.blocks.25.attn.qkv.lora_a.default.weight  Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 1280]) torch.Size([64, 1280]) 
base_model.model.visual.blocks.25.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:
torch.Size([64, 1280])  Adam-mini found the param block with name:
 torch.Size([64, 1280])base_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weight
 Adam-mini found the param block with name:base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3420, 64])base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weight  base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight 
 base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weighttorch.Size([64, 1280]) 
torch.Size([3840, 64]) torch.Size([3420, 64])
Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name:
  base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weight
base_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weight  torch.Size([64, 3420])torch.Size([3420, 64])Adam-mini found the param block with name:

 Adam-mini found the param block with name:base_model.model.visual.blocks.25.attn.proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:    torch.Size([64, 1280])base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weight
base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.25.attn.proj.lora_a.default.weight  Adam-mini found the param block with name: torch.Size([1280, 64]) Adam-mini found the param block with name:
torch.Size([64, 1280])torch.Size([64, 1280])base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weight 

 base_model.model.visual.blocks.25.attn.proj.lora_b.default.weight torch.Size([64, 1280])torch.Size([1280, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.25.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:    base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weighttorch.Size([1280, 64])Adam-mini found the param block with name:torch.Size([3420, 64])base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weight  

 torch.Size([64, 1280])base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weighttorch.Size([3420, 64]) 

torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name: Adam-mini found the param block with name:
base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weight   base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([64, 1280])  torch.Size([64, 3420])torch.Size([3420, 64])
Adam-mini found the param block with name:base_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weight
 
 Adam-mini found the param block with name:base_model.model.visual.blocks.22.attn.proj.lora_a.default.weighttorch.Size([64, 3420])  Adam-mini found the param block with name:
base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])  
base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3420, 64])  base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:
base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weighttorch.Size([1280, 64])   
torch.Size([64, 1280])base_model.model.visual.blocks.22.attn.proj.lora_b.default.weighttorch.Size([1280, 64])

 Adam-mini found the param block with name: torch.Size([1280, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weight 
 base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weight torch.Size([64, 1280])torch.Size([3420, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.12.attn.qkv.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weight torch.Size([3420, 64])   
base_model.model.visual.blocks.7.attn.qkv.lora_a.default.weighttorch.Size([64, 1280]) 
base_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weighttorch.Size([64, 1280])torch.Size([64, 1280]) 
Adam-mini found the param block with name:
Adam-mini found the param block with name:torch.Size([64, 3420]) Adam-mini found the param block with name: 
Adam-mini found the param block with name:base_model.model.visual.blocks.12.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weight  base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([64, 3420]) 
base_model.model.visual.blocks.7.attn.qkv.lora_b.default.weight torch.Size([3840, 64])torch.Size([3420, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weight  
torch.Size([3840, 64])base_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weighttorch.Size([1280, 64])

 torch.Size([1280, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.visual.blocks.12.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([64, 1280]) 

base_model.model.visual.blocks.7.attn.proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:
  Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.26.attn.qkv.lora_a.default.weight base_model.model.visual.blocks.12.attn.proj.lora_b.default.weightAdam-mini found the param block with name:    base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weighttorch.Size([64, 1280])torch.Size([64, 1280])base_model.model.visual.blocks.7.attn.proj.lora_b.default.weighttorch.Size([1280, 64]) 


 torch.Size([3420, 64])
torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.26.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.26.attn.qkv.lora_b.default.weight  torch.Size([3840, 64])torch.Size([3840, 64])

Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3420]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weight  base_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name: Adam-mini found the param block with name:
  base_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weight torch.Size([64, 1280]) base_model.model.visual.blocks.26.attn.proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.26.attn.proj.lora_a.default.weight
torch.Size([1280, 64])   
torch.Size([64, 1280])base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name: 
 
torch.Size([3420, 64])base_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([3420, 64])base_model.model.visual.blocks.26.attn.proj.lora_b.default.weightAdam-mini found the param block with name:
 Adam-mini found the param block with name:  torch.Size([1280, 64])base_model.model.visual.blocks.26.attn.proj.lora_b.default.weight
 base_model.model.visual.blocks.23.attn.qkv.lora_a.default.weightAdam-mini found the param block with name: torch.Size([1280, 64]) torch.Size([64, 1280])

Adam-mini found the param block with name:base_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weight  base_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 1280]) Adam-mini found the param block with name: 
torch.Size([64, 1280]) base_model.model.visual.blocks.23.attn.qkv.lora_b.default.weight
Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weight    torch.Size([3840, 64])base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weight
  torch.Size([64, 1280]) torch.Size([64, 1280])torch.Size([3420, 64])
base_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weight

 Adam-mini found the param block with name:torch.Size([3420, 64])Adam-mini found the param block with name: Adam-mini found the param block with name:
 base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.23.attn.proj.lora_a.default.weight base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([3420, 64])torch.Size([3420, 64])
 
Adam-mini found the param block with name:base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weight
Adam-mini found the param block with name:  base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])base_model.model.visual.blocks.23.attn.proj.lora_b.default.weight 
 Adam-mini found the param block with name:torch.Size([64, 3420])Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weight  
 base_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weighttorch.Size([64, 1280])base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weight 
Adam-mini found the param block with name: torch.Size([64, 1280])Adam-mini found the param block with name: torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weight
  Adam-mini found the param block with name: torch.Size([1280, 64])base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight 
torch.Size([64, 1280])base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight 
 torch.Size([3420, 64])
torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3420])  base_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.13.attn.qkv.lora_b.default.weighttorch.Size([64, 3420])
 base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weighttorch.Size([3840, 64])  
Adam-mini found the param block with name:
torch.Size([64, 1280])torch.Size([64, 1280]) 

Adam-mini found the param block with name:base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weight  base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([1280, 64])  
torch.Size([1280, 64]) Adam-mini found the param block with name:
base_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.8.attn.qkv.lora_b.default.weight  base_model.model.visual.blocks.13.attn.proj.lora_a.default.weight torch.Size([3420, 64]) torch.Size([3840, 64])

torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:
   base_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.13.attn.proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weight   base_model.model.visual.blocks.8.attn.proj.lora_a.default.weight torch.Size([64, 3420])torch.Size([1280, 64])
base_model.model.visual.blocks.27.attn.qkv.lora_b.default.weight
  torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([64, 1280])
torch.Size([3840, 64]) 

base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.27.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:torch.Size([1280, 64]) 
 torch.Size([3840, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.8.attn.proj.lora_b.default.weight
   base_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.27.attn.proj.lora_a.default.weighttorch.Size([1280, 64]) 
 torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([64, 1280])
 Adam-mini found the param block with name:
base_model.model.visual.blocks.24.attn.qkv.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.27.attn.proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])  Adam-mini found the param block with name: 
base_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 1280]) base_model.model.visual.blocks.27.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  
base_model.model.visual.blocks.24.attn.qkv.lora_b.default.weighttorch.Size([3420, 64]) torch.Size([64, 1280]) torch.Size([1280, 64])

Adam-mini found the param block with name:torch.Size([3840, 64])
 
base_model.model.visual.blocks.27.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weighttorch.Size([1280, 64]) 
torch.Size([3420, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.24.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 1280]) torch.Size([64, 1280])
 
Adam-mini found the param block with name:base_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 1280])  base_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weight
torch.Size([64, 1280])Adam-mini found the param block with name:
 Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280]) Adam-mini found the param block with name:   
base_model.model.visual.blocks.24.attn.proj.lora_b.default.weighttorch.Size([3420, 64])base_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weight 
  Adam-mini found the param block with name:torch.Size([1280, 64])torch.Size([3420, 64]) torch.Size([3420, 64])

base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([3420, 64])base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name: 
 torch.Size([64, 1280])base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: 
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])   
base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weight base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([64, 3420])base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 3420])torch.Size([3420, 64])
  

torch.Size([3420, 64])torch.Size([3420, 64])Adam-mini found the param block with name:

 Adam-mini found the param block with name:base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weight  base_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weighttorch.Size([1280, 64]) Adam-mini found the param block with name:torch.Size([1280, 64])
Adam-mini found the param block with name:
  base_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 3420])torch.Size([64, 1280])
base_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weight
 torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weight base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weight   base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weight base_model.model.visual.blocks.14.attn.qkv.lora_a.default.weighttorch.Size([3420, 64])torch.Size([1280, 64]) 
torch.Size([1280, 64]) 
torch.Size([64, 1280])
torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 3420]) base_model.model.visual.blocks.9.attn.qkv.lora_b.default.weight
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weight  torch.Size([3840, 64])Adam-mini found the param block with name:torch.Size([3840, 64])base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:

 base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weight torch.Size([64, 1280]) base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weight
 torch.Size([1280, 64])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.14.attn.proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.9.attn.proj.lora_a.default.weight    base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([3840, 64])torch.Size([64, 1280])
 
 torch.Size([3840, 64])
base_model.model.visual.blocks.25.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:
  torch.Size([64, 1280])base_model.model.visual.blocks.14.attn.proj.lora_b.default.weightAdam-mini found the param block with name:
  Adam-mini found the param block with name:base_model.model.visual.blocks.9.attn.proj.lora_b.default.weight torch.Size([1280, 64])Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.proj.lora_a.default.weightAdam-mini found the param block with name:
 torch.Size([1280, 64])  
base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.28.attn.proj.lora_a.default.weighttorch.Size([64, 1280])  
torch.Size([64, 1280])torch.Size([3840, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.28.attn.proj.lora_b.default.weighttorch.Size([64, 1280])  
base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight base_model.model.visual.blocks.28.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.25.attn.proj.lora_a.default.weight torch.Size([1280, 64])  
Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([64, 1280]) torch.Size([1280, 64])

base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight
 Adam-mini found the param block with name:torch.Size([3420, 64]) Adam-mini found the param block with name:
Adam-mini found the param block with name:base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weight   base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.25.attn.proj.lora_b.default.weighttorch.Size([3420, 64])  Adam-mini found the param block with name:torch.Size([1280, 64])torch.Size([64, 1280]) 

base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weight
Adam-mini found the param block with name:  base_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weighttorch.Size([64, 1280]) 
Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([3420, 64])   base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weight
base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weight torch.Size([64, 1280])  torch.Size([3420, 64])
torch.Size([3420, 64])torch.Size([64, 1280])
Adam-mini found the param block with name:
 
base_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 1280])base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: 
torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:    base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3420])base_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:
   torch.Size([3420, 64])torch.Size([3420, 64])torch.Size([64, 1280])
base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weight
Adam-mini found the param block with name:
  base_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 1280]) torch.Size([1280, 64])
base_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:
 Adam-mini found the param block with name:  torch.Size([3420, 64]) base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weight
base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weight   torch.Size([64, 3420])torch.Size([64, 3420])
torch.Size([3420, 64])
Adam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weight    Adam-mini found the param block with name:base_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weighttorch.Size([64, 1280]) torch.Size([64, 3420])base_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weight  
torch.Size([1280, 64])
 torch.Size([1280, 64])
torch.Size([64, 3420])
Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weight   base_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weighttorch.Size([3840, 64]) torch.Size([1280, 64])
torch.Size([1280, 64])
Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.10.attn.qkv.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([64, 1280]) 
base_model.model.visual.blocks.15.attn.proj.lora_a.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name:   
 base_model.model.visual.blocks.29.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.26.attn.qkv.lora_a.default.weight base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.10.attn.qkv.lora_b.default.weightAdam-mini found the param block with name: torch.Size([3840, 64])   torch.Size([64, 1280])torch.Size([3840, 64])torch.Size([64, 1280])

base_model.model.visual.blocks.15.attn.proj.lora_b.default.weight
 
Adam-mini found the param block with name:torch.Size([1280, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.29.attn.qkv.lora_b.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.26.attn.qkv.lora_b.default.weighttorch.Size([3840, 64])   
base_model.model.visual.blocks.29.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.10.attn.proj.lora_a.default.weighttorch.Size([3840, 64])Adam-mini found the param block with name: 
  torch.Size([64, 1280])base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 1280])
torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.29.attn.proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.visual.blocks.10.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.29.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.26.attn.proj.lora_a.default.weight  torch.Size([64, 1280]) torch.Size([1280, 64])torch.Size([1280, 64])torch.Size([3420, 64])torch.Size([64, 1280])




Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_b.default.weight torch.Size([1280, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.26.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280]) 
base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weight  
base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weighttorch.Size([3420, 64])Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:  base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])
base_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weight 
 torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3420, 64]) Adam-mini found the param block with name: 
base_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weight base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])torch.Size([3420, 64]) 
 torch.Size([3420, 64])Adam-mini found the param block with name:
base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weight 
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 1280])base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weight
  torch.Size([64, 3420])torch.Size([3420, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:

Adam-mini found the param block with name:   base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weight torch.Size([3420, 64]) base_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weighttorch.Size([64, 1280])torch.Size([64, 1280])
Adam-mini found the param block with name:

  torch.Size([1280, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight
   base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3420])base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name:
 torch.Size([3420, 64])torch.Size([3420, 64])base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weight
Adam-mini found the param block with name:
  base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3420]) 
Adam-mini found the param block with name:torch.Size([1280, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:   base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weight  torch.Size([64, 3420])base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight 

 Adam-mini found the param block with name:torch.Size([1280, 64])torch.Size([64, 3420])Adam-mini found the param block with name:
 base_model.model.visual.blocks.30.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:
   base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.visual.blocks.16.attn.qkv.lora_b.default.weight 
torch.Size([1280, 64]) base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weight
Adam-mini found the param block with name:torch.Size([3840, 64])  
base_model.model.visual.blocks.30.attn.qkv.lora_b.default.weighttorch.Size([1280, 64]) 
Adam-mini found the param block with name:torch.Size([3840, 64]) 
base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) Adam-mini found the param block with name:
base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.16.attn.proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([64, 1280])
  torch.Size([64, 1280])base_model.model.visual.blocks.11.attn.qkv.lora_b.default.weightbase_model.model.visual.blocks.30.attn.proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: 
   torch.Size([64, 1280])torch.Size([3840, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.30.attn.qkv.lora_a.default.weight
base_model.model.visual.blocks.27.attn.qkv.lora_b.default.weight
 base_model.model.visual.blocks.16.attn.proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([64, 1280]) torch.Size([3840, 64])base_model.model.visual.blocks.30.attn.proj.lora_b.default.weight

 Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name:torch.Size([1280, 64]) 
 base_model.model.visual.blocks.11.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.30.attn.qkv.lora_b.default.weight
  Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([3840, 64]) 
base_model.model.visual.blocks.27.attn.proj.lora_a.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 1280])  base_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.11.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weight
   Adam-mini found the param block with name:torch.Size([1280, 64])torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([64, 1280]) 


 base_model.model.visual.blocks.30.attn.proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.27.attn.proj.lora_b.default.weight Adam-mini found the param block with name:  base_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280]) torch.Size([1280, 64])
 
base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([3420, 64]) torch.Size([3420, 64])
Adam-mini found the param block with name:
base_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weight  base_model.model.visual.blocks.30.attn.proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])  
base_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weighttorch.Size([1280, 64])Adam-mini found the param block with name:
  Adam-mini found the param block with name:base_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 1280]) base_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weighttorch.Size([64, 1280])
base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight
  Adam-mini found the param block with name:torch.Size([3420, 64])torch.Size([64, 1280])Adam-mini found the param block with name: Adam-mini found the param block with name:

 base_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weight base_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([3420, 64]) 

 Adam-mini found the param block with name:torch.Size([3420, 64])base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weight
 Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weight torch.Size([3420, 64]) Adam-mini found the param block with name:
 base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weight
Adam-mini found the param block with name:   torch.Size([3420, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 1280])
  base_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weight 
torch.Size([64, 3420]) base_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weight
torch.Size([3420, 64]) Adam-mini found the param block with name:
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3420])  
 base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([3420, 64])  Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([1280, 64])base_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weight

 
 base_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weight torch.Size([1280, 64])torch.Size([64, 3420])

Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 3420])torch.Size([1280, 64]) 
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.qkv.lora_a.default.weight
 base_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:   base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weighttorch.Size([64, 1280])torch.Size([64, 3420])base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weight 

 torch.Size([64, 1280])
torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weight base_model.model.visual.blocks.31.attn.qkv.lora_b.default.weight base_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.12.attn.qkv.lora_a.default.weight torch.Size([3840, 64])Adam-mini found the param block with name:   torch.Size([3840, 64])
torch.Size([64, 1280])base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weight

torch.Size([1280, 64]) 
torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.qkv.lora_b.default.weight Adam-mini found the param block with name: torch.Size([3840, 64])base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:
Adam-mini found the param block with name:   base_model.model.visual.blocks.31.attn.proj.lora_a.default.weighttorch.Size([3840, 64])base_model.model.visual.blocks.17.attn.proj.lora_a.default.weight
  Adam-mini found the param block with name:torch.Size([64, 1280]) torch.Size([64, 1280])
Adam-mini found the param block with name:
base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([64, 1280])   
base_model.model.visual.blocks.31.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.17.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.28.attn.proj.lora_a.default.weighttorch.Size([64, 1280])  torch.Size([1280, 64])torch.Size([1280, 64])
 

Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.31.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name:
  Adam-mini found the param block with name:base_model.model.visual.blocks.28.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.12.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([1280, 64])base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weight torch.Size([1280, 64]) 

 base_model.model.visual.blocks.31.attn.proj.lora_a.default.weighttorch.Size([64, 1280])torch.Size([64, 1280]) 

torch.Size([64, 1280])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3420, 64]) 
base_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.31.attn.proj.lora_b.default.weight  base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weight base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weight torch.Size([3420, 64])torch.Size([1280, 64]) torch.Size([64, 1280])
torch.Size([64, 1280])
Adam-mini found the param block with name:

 base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weighttorch.Size([3420, 64]) 
  Adam-mini found the param block with name:base_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weighttorch.Size([3420, 64])Adam-mini found the param block with name:  base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weight 
torch.Size([3420, 64])base_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280]) 

torch.Size([64, 1280])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weighttorch.Size([3420, 64]) torch.Size([3420, 64])
torch.Size([3420, 64])torch.Size([64, 1280]) 


torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:  torch.Size([3420, 64])  torch.Size([64, 3420])

torch.Size([1280, 64])base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight
base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([64, 1280]) base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weight
torch.Size([64, 3420]) 
Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name: 
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weight  base_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3420])  
torch.Size([3420, 64])torch.Size([1280, 64])Adam-mini found the param block with name:

 Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weight torch.Size([64, 3584]) base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weight
torch.Size([1280, 64]) Adam-mini found the param block with name:
torch.Size([64, 1280]) Adam-mini found the param block with name:Adam-mini found the param block with name:
base_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weight  base_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weight base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weight Adam-mini found the param block with name: torch.Size([64, 3420]) torch.Size([3584, 64])
torch.Size([64, 1280])base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weight

 Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3840, 64])
  base_model.model.visual.blocks.13.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.29.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:    torch.Size([64, 1280])torch.Size([1280, 64])torch.Size([3840, 64])Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weight
 

base_model.model.visual.blocks.18.attn.proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([64, 1280]) 

base_model.model.visual.blocks.13.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name:Adam-mini found the param block with name: 
  base_model.model.visual.blocks.18.attn.proj.lora_b.default.weightbase_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weightbase_model.model.visual.blocks.29.attn.proj.lora_a.default.weight   torch.Size([1280, 64])torch.Size([64, 1280])torch.Size([512, 64])


Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.13.attn.proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.visual.blocks.29.attn.proj.lora_b.default.weight   Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weighttorch.Size([1280, 64]) 

 base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weighttorch.Size([64, 3584])base_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
Adam-mini found the param block with name: 
torch.Size([64, 1280]) base_model.model.visual.blocks.13.attn.proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weight
   Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weighttorch.Size([1280, 64])torch.Size([64, 1280])Adam-mini found the param block with name:   
base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weighttorch.Size([3584, 64])
base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weight
  Adam-mini found the param block with name:torch.Size([512, 64]) torch.Size([3420, 64])
base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weight torch.Size([3420, 64]) base_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weight
Adam-mini found the param block with name:torch.Size([64, 1280]) Adam-mini found the param block with name:
  torch.Size([64, 3584])base_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weightbase_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: 
 Adam-mini found the param block with name:  torch.Size([64, 1280])torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weight
 
  torch.Size([3420, 64])base_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weight
Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name:  
 base_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weighttorch.Size([512, 64])base_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:
 torch.Size([3584, 64]) 
Adam-mini found the param block with name:torch.Size([3420, 64]) base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weight
base_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weight  torch.Size([3420, 64])torch.Size([64, 1280])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584]) Adam-mini found the param block with name:
  torch.Size([64, 3584]) 
base_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight   Adam-mini found the param block with name: torch.Size([3420, 64])torch.Size([64, 3420])torch.Size([64, 3420]) base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weight
base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weight
 
 torch.Size([18944, 64])
torch.Size([512, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([1280, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:base_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weight   Adam-mini found the param block with name:base_model.model.model.layers.0.mlp.up_proj.lora_a.default.weighttorch.Size([1280, 64])base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weight 
 base_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weight  torch.Size([64, 3420])torch.Size([64, 3584])torch.Size([64, 3584])


Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.0.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name: 
base_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weight  torch.Size([18944, 64])Adam-mini found the param block with name: torch.Size([1280, 64])base_model.model.visual.blocks.19.attn.qkv.lora_a.default.weight 
base_model.model.visual.blocks.30.attn.qkv.lora_b.default.weighttorch.Size([3584, 64])  

torch.Size([3840, 64])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name: 
 Adam-mini found the param block with name:base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.30.attn.proj.lora_a.default.weight   Adam-mini found the param block with name:torch.Size([3584, 64])base_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight
torch.Size([64, 1280])  base_model.model.visual.blocks.14.attn.qkv.lora_a.default.weight
torch.Size([64, 3584])Adam-mini found the param block with name: 
 Adam-mini found the param block with name:torch.Size([64, 1280])base_model.model.visual.blocks.19.attn.proj.lora_a.default.weight 
 base_model.model.visual.blocks.30.attn.proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([64, 1280])  Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weighttorch.Size([1280, 64])base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weight
  base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weight
 Adam-mini found the param block with name:torch.Size([64, 3584]) torch.Size([18944, 64])
 torch.Size([3840, 64])

base_model.model.visual.blocks.19.attn.proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weightbase_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name: torch.Size([64, 1280])torch.Size([3584, 64])base_model.model.model.layers.0.mlp.up_proj.lora_a.default.weight 
 
base_model.model.visual.blocks.14.attn.proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: Adam-mini found the param block with name:
torch.Size([64, 1280])  
Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight  base_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.0.mlp.up_proj.lora_b.default.weighttorch.Size([3420, 64])Adam-mini found the param block with name:  
 torch.Size([64, 3584]) torch.Size([18944, 64])torch.Size([64, 1280])

base_model.model.visual.blocks.14.attn.proj.lora_b.default.weight
 Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name: 
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weight base_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weight torch.Size([512, 64])  torch.Size([64, 1280])
Adam-mini found the param block with name:
base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weighttorch.Size([3420, 64])  
base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 18944])Adam-mini found the param block with name:Adam-mini found the param block with name:  
 base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weighttorch.Size([64, 1280])base_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weight  Adam-mini found the param block with name:
Adam-mini found the param block with name:torch.Size([3420, 64])torch.Size([64, 3584]) 
 
base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:  torch.Size([3584, 64]) torch.Size([64, 1280])base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight

Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weight base_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:torch.Size([512, 64])  

base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3420]) 
torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weightbase_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:    Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weightbase_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weighttorch.Size([1280, 64]) torch.Size([64, 3584]) torch.Size([64, 1280]) 

torch.Size([64, 3584])base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weight

 Adam-mini found the param block with name:torch.Size([64, 3420]) 
Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weight  base_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weight Adam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([3420, 64])  
torch.Size([3584, 64])base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weighttorch.Size([3584, 64]) base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weight

torch.Size([64, 1280]) 
Adam-mini found the param block with name:torch.Size([1280, 64])
 base_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])Adam-mini found the param block with name:
 base_model.model.visual.blocks.31.attn.qkv.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3840, 64])Adam-mini found the param block with name:
  base_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 3584])base_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584])
Adam-mini found the param block with name: 
 torch.Size([1280, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.31.attn.proj.lora_a.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280]) base_model.model.visual.blocks.20.attn.qkv.lora_a.default.weight 
base_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weight  torch.Size([18944, 64])Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([512, 64])

 
base_model.model.visual.blocks.31.attn.proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([1280, 64]) base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weight
base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 1280]) torch.Size([3840, 64])
 base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weight
Adam-mini found the param block with name:  Adam-mini found the param block with name: torch.Size([64, 3584]) base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])
 
base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: 
  base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weighttorch.Size([3840, 64])base_model.model.visual.blocks.20.attn.proj.lora_a.default.weight 
 base_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([18944, 64]) torch.Size([64, 1280]) 
torch.Size([512, 64])
base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([3420, 64])base_model.model.visual.blocks.15.attn.proj.lora_a.default.weight 
 Adam-mini found the param block with name:base_model.model.visual.blocks.20.attn.proj.lora_b.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name: 
  Adam-mini found the param block with name:base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weight torch.Size([1280, 64])base_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight 
  torch.Size([64, 18944])base_model.model.visual.blocks.15.attn.proj.lora_b.default.weighttorch.Size([64, 1280])torch.Size([64, 3584])
 

torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.1.mlp.down_proj.lora_b.default.weight base_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weightbase_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([3584, 64])  
 torch.Size([3584, 64])base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weighttorch.Size([3420, 64])torch.Size([64, 1280]) 


torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:base_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([3420, 64]) torch.Size([64, 3584])base_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weight torch.Size([3420, 64])

 base_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weight
torch.Size([64, 3584]) Adam-mini found the param block with name:
 torch.Size([64, 3420])base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weight
 torch.Size([3584, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weight base_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weighttorch.Size([18944, 64])  
base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weight torch.Size([64, 1280])torch.Size([1280, 64])torch.Size([64, 1280])
 

torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weighttorch.Size([3420, 64])  base_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weight 
torch.Size([64, 3584]) torch.Size([3420, 64])

torch.Size([512, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weight  base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])  
torch.Size([18944, 64])base_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:
 Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weight   base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weight torch.Size([64, 3420])torch.Size([64, 3420])Adam-mini found the param block with name:
torch.Size([64, 3584]) 
 
torch.Size([3584, 64])base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([64, 18944])  base_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weightbase_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weight
base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weight  Adam-mini found the param block with name: torch.Size([512, 64])torch.Size([1280, 64]) Adam-mini found the param block with name:torch.Size([1280, 64])
base_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weight

  base_model.model.model.layers.1.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weight  torch.Size([64, 3584])torch.Size([512, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.21.attn.qkv.lora_a.default.weightbase_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weight  Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 1280])  
base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weight  torch.Size([3584, 64])torch.Size([64, 3584])torch.Size([64, 1280])
torch.Size([64, 3584])Adam-mini found the param block with name:


 Adam-mini found the param block with name:base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weight Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3840, 64])  base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weight torch.Size([3584, 64])
base_model.model.visual.blocks.16.attn.qkv.lora_b.default.weight base_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weight
 torch.Size([512, 64]) torch.Size([3840, 64])
torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 1280]) 
Adam-mini found the param block with name:base_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([18944, 64]) 
  base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])base_model.model.visual.blocks.16.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.21.attn.proj.lora_b.default.weight  
 torch.Size([1280, 64])torch.Size([64, 3584])torch.Size([64, 1280])Adam-mini found the param block with name:
Adam-mini found the param block with name:
 
 base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584])Adam-mini found the param block with name:

  base_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.16.attn.proj.lora_b.default.weight Adam-mini found the param block with name:  base_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight torch.Size([1280, 64]) base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weighttorch.Size([512, 64])Adam-mini found the param block with name:
torch.Size([64, 1280]) 
 
torch.Size([18944, 64])base_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:
  base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weight  base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])
  torch.Size([18944, 64])torch.Size([64, 1280])
Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weightbase_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])torch.Size([512, 64]) 
 
torch.Size([3420, 64])base_model.model.model.layers.0.mlp.up_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([64, 3584]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weighttorch.Size([3420, 64])Adam-mini found the param block with name:  
 base_model.model.model.layers.0.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weight 
 torch.Size([18944, 64])
torch.Size([64, 1280])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weight   base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weighttorch.Size([3584, 64])base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3420]) Adam-mini found the param block with name:
  
torch.Size([64, 18944])base_model.model.model.layers.2.mlp.down_proj.lora_a.default.weighttorch.Size([3420, 64])
Adam-mini found the param block with name:
  base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 18944])  
Adam-mini found the param block with name:torch.Size([1280, 64])base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weight 
base_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([3584, 64]) 
 torch.Size([64, 3584])base_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.2.mlp.down_proj.lora_b.default.weight
  torch.Size([64, 3420])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64])Adam-mini found the param block with name: 
 base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weight   base_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weighttorch.Size([64, 1280])torch.Size([64, 3584])Adam-mini found the param block with name: 
Adam-mini found the param block with name:
  torch.Size([1280, 64])base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:
Adam-mini found the param block with name:base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weight    base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weight torch.Size([64, 3584])torch.Size([3584, 64])


 Adam-mini found the param block with name:torch.Size([3840, 64]) Adam-mini found the param block with name:Adam-mini found the param block with name:
base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weightbase_model.model.visual.blocks.17.attn.qkv.lora_a.default.weight torch.Size([18944, 64]) base_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weight 
torch.Size([3584, 64])torch.Size([64, 1280]) Adam-mini found the param block with name:

torch.Size([64, 3584]) 
base_model.model.visual.blocks.22.attn.proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:torch.Size([3840, 64])

 Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weight  base_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([512, 64])  
torch.Size([64, 3584])base_model.model.visual.blocks.22.attn.proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([1280, 64])base_model.model.visual.blocks.17.attn.proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([64, 1280])Adam-mini found the param block with name:base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weight
  base_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weighttorch.Size([512, 64]) Adam-mini found the param block with name:
torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.visual.blocks.17.attn.proj.lora_b.default.weight
  base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weight torch.Size([1280, 64])torch.Size([64, 1280])
Adam-mini found the param block with name:
 base_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weighttorch.Size([512, 64])  
base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])  
base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weighttorch.Size([3420, 64]) 
torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:
  base_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([512, 64])torch.Size([64, 3584]) 
Adam-mini found the param block with name:
base_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weight  base_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3420, 64]) 
  Adam-mini found the param block with name:torch.Size([64, 1280])base_model.model.model.layers.2.mlp.down_proj.lora_a.default.weight
base_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weight   base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 18944])torch.Size([3584, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:
 
  torch.Size([64, 3584])base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weight 
 Adam-mini found the param block with name:torch.Size([3420, 64]) torch.Size([64, 1280])Adam-mini found the param block with name:
Adam-mini found the param block with name:
base_model.model.model.layers.2.mlp.down_proj.lora_b.default.weight   base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weight  torch.Size([3584, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584])

  
base_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weight  torch.Size([64, 3420])torch.Size([3420, 64])Adam-mini found the param block with name:

 base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weightbase_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  torch.Size([64, 3584]) torch.Size([1280, 64])base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weight
base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weight
base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weight   torch.Size([64, 3420])torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3584])
 

base_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weight   base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.23.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:torch.Size([1280, 64])torch.Size([18944, 64])torch.Size([3584, 64]) 
 

base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weighttorch.Size([64, 1280]) 
torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.23.attn.qkv.lora_b.default.weight    torch.Size([18944, 64])Adam-mini found the param block with name:torch.Size([3840, 64])base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weight
 
  base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weighttorch.Size([64, 3584])torch.Size([64, 18944]) 

torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_a.default.weight   base_model.model.visual.blocks.23.attn.proj.lora_a.default.weightbase_model.model.model.layers.1.mlp.down_proj.lora_b.default.weight  torch.Size([64, 18944])base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weight
base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weight  torch.Size([64, 1280]) torch.Size([3584, 64])
Adam-mini found the param block with name:torch.Size([512, 64])torch.Size([3840, 64])
 

Adam-mini found the param block with name:base_model.model.model.layers.3.mlp.down_proj.lora_b.default.weight  base_model.model.visual.blocks.23.attn.proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([1280, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weightbase_model.model.visual.blocks.18.attn.proj.lora_a.default.weightbase_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:  torch.Size([64, 3584])torch.Size([64, 1280]) 
base_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])

base_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 1280])Adam-mini found the param block with name: torch.Size([64, 3584]) 
 base_model.model.visual.blocks.18.attn.proj.lora_b.default.weightbase_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weight
base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weight Adam-mini found the param block with name:  torch.Size([3584, 64])Adam-mini found the param block with name:torch.Size([1280, 64]) 
base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weighttorch.Size([512, 64])
  
base_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weighttorch.Size([3420, 64]) 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weight
   base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584]) base_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weight
  Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([64, 3584])torch.Size([64, 1280])Adam-mini found the param block with name: 


 base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([512, 64])  torch.Size([3420, 64])base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weight
base_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weight
  torch.Size([3584, 64]) torch.Size([3420, 64])

torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:   torch.Size([512, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weightbase_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weight
  base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weight  torch.Size([64, 3420]) torch.Size([64, 1280])torch.Size([64, 3584])Adam-mini found the param block with name:
torch.Size([64, 3584]) 


base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: 
base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weight   Adam-mini found the param block with name: torch.Size([1280, 64]) base_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weight torch.Size([512, 64]) 
 torch.Size([3584, 64])
torch.Size([3420, 64])torch.Size([18944, 64])


Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:   torch.Size([64, 3584])base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weight base_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weight
base_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weightbase_model.model.visual.blocks.24.attn.qkv.lora_a.default.weight  Adam-mini found the param block with name: torch.Size([64, 3420]) 
torch.Size([64, 3584])torch.Size([64, 1280])torch.Size([3584, 64]) 

Adam-mini found the param block with name:
base_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([18944, 64])   
base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weighttorch.Size([1280, 64])base_model.model.visual.blocks.24.attn.qkv.lora_b.default.weight
Adam-mini found the param block with name:   torch.Size([18944, 64])base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weighttorch.Size([3840, 64])Adam-mini found the param block with name:
  
torch.Size([64, 3584])base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([18944, 64])base_model.model.visual.blocks.19.attn.qkv.lora_a.default.weight
  torch.Size([18944, 64]) 
base_model.model.model.layers.3.mlp.down_proj.lora_a.default.weightbase_model.model.visual.blocks.24.attn.proj.lora_a.default.weighttorch.Size([64, 1280]) 
 Adam-mini found the param block with name:torch.Size([64, 18944])torch.Size([64, 1280]) 
Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.4.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.3.mlp.down_proj.lora_b.default.weight torch.Size([64, 3584]) 
 torch.Size([3840, 64])torch.Size([3584, 64])
base_model.model.visual.blocks.24.attn.proj.lora_b.default.weight
Adam-mini found the param block with name:  base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weighttorch.Size([1280, 64]) 
torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.4.mlp.down_proj.lora_a.default.weight base_model.model.visual.blocks.19.attn.proj.lora_b.default.weight base_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weight torch.Size([64, 18944])  
torch.Size([64, 1280])torch.Size([1280, 64])torch.Size([64, 3584])


Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3584, 64]) 
base_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weighttorch.Size([3420, 64])Adam-mini found the param block with name: 
 torch.Size([3584, 64])Adam-mini found the param block with name:base_model.model.model.layers.2.mlp.down_proj.lora_a.default.weight
  base_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:torch.Size([64, 1280])
 Adam-mini found the param block with name:
Adam-mini found the param block with name:base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weight   Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weighttorch.Size([64, 3584])base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weight   
 base_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])base_model.model.model.layers.2.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584]) 

torch.Size([3420, 64]) Adam-mini found the param block with name:
torch.Size([3584, 64]) Adam-mini found the param block with name:Adam-mini found the param block with name:
base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weight   base_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weightbase_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64]) Adam-mini found the param block with name: 
torch.Size([3420, 64]) torch.Size([512, 64])base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight

Adam-mini found the param block with name:  base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 1280]) Adam-mini found the param block with name:
 torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name: 
 Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weight  
  torch.Size([64, 3584])base_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weighttorch.Size([3420, 64])torch.Size([64, 3420])
 

Adam-mini found the param block with name:torch.Size([3584, 64]) Adam-mini found the param block with name:
base_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64]) Adam-mini found the param block with name:
torch.Size([512, 64]) base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weight
 Adam-mini found the param block with name:base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weight torch.Size([1280, 64]) base_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weight
torch.Size([64, 3420])Adam-mini found the param block with name: 
 torch.Size([64, 3584])base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:
 Adam-mini found the param block with name: torch.Size([64, 3584]) base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weight
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weight  torch.Size([1280, 64]) Adam-mini found the param block with name:base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weighttorch.Size([512, 64]) 
base_model.model.visual.blocks.25.attn.qkv.lora_a.default.weight 
base_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weight torch.Size([512, 64]) torch.Size([64, 1280])
torch.Size([3584, 64])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weight  base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])  
base_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weighttorch.Size([3840, 64])Adam-mini found the param block with name: 
Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weight
Adam-mini found the param block with name:   torch.Size([64, 3584])base_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.visual.blocks.20.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:torch.Size([512, 64])
   
Adam-mini found the param block with name:base_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weighttorch.Size([64, 1280])base_model.model.visual.blocks.25.attn.proj.lora_a.default.weight   
torch.Size([3584, 64])base_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 1280]) 
 Adam-mini found the param block with name:
torch.Size([18944, 64])base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weight 
 base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584]) Adam-mini found the param block with name: 
torch.Size([3840, 64]) base_model.model.visual.blocks.25.attn.proj.lora_b.default.weight
base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name: torch.Size([64, 3584])torch.Size([1280, 64]) base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weight
base_model.model.model.layers.4.mlp.up_proj.lora_a.default.weight
Adam-mini found the param block with name:   Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584])base_model.model.visual.blocks.20.attn.proj.lora_a.default.weight 
base_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weight 
 torch.Size([64, 1280])torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: 
 Adam-mini found the param block with name:base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.20.attn.proj.lora_b.default.weighttorch.Size([64, 1280])base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weighttorch.Size([18944, 64])Adam-mini found the param block with name: 
 
 torch.Size([1280, 64])base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weighttorch.Size([64, 3584])
 
Adam-mini found the param block with name:torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name:   base_model.model.model.layers.4.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([3420, 64])Adam-mini found the param block with name:  base_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weight
base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weight  torch.Size([64, 18944])torch.Size([18944, 64])
base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weight 
Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([18944, 64])  torch.Size([64, 1280])base_model.model.model.layers.4.mlp.down_proj.lora_b.default.weight
base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weight  
Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 1280]) 
Adam-mini found the param block with name:base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weight
Adam-mini found the param block with name:   base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.5.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: 
torch.Size([3420, 64])  
torch.Size([64, 18944])base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:
  torch.Size([3420, 64])Adam-mini found the param block with name:base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weight
  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weighttorch.Size([18944, 64])  
 base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weight  torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([3584, 64])
torch.Size([64, 1280]) 
Adam-mini found the param block with name:
base_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.model.layers.3.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3420])base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weight  
 torch.Size([64, 18944])base_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:
torch.Size([3584, 64])  
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3420, 64])base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weight  
 base_model.model.model.layers.3.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weight torch.Size([64, 3584]) torch.Size([3584, 64])Adam-mini found the param block with name:
torch.Size([1280, 64])
 
Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weight   base_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weightbase_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3584])  
torch.Size([3584, 64])torch.Size([64, 3420])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([512, 64])torch.Size([64, 3584])
base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.26.attn.qkv.lora_a.default.weight base_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weight 
 torch.Size([1280, 64])torch.Size([64, 3584])
torch.Size([64, 1280])Adam-mini found the param block with name:
 Adam-mini found the param block with name:
base_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:   base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([3584, 64])  torch.Size([512, 64])
torch.Size([64, 3584])base_model.model.visual.blocks.26.attn.qkv.lora_b.default.weight

Adam-mini found the param block with name:  base_model.model.visual.blocks.21.attn.qkv.lora_a.default.weighttorch.Size([3840, 64])
 Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weight torch.Size([512, 64])base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:  
base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weight torch.Size([64, 3584])torch.Size([64, 3584]) base_model.model.visual.blocks.26.attn.proj.lora_a.default.weight

torch.Size([3840, 64]) 
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([64, 1280])  base_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:
  torch.Size([512, 64]) 
base_model.model.visual.blocks.21.attn.proj.lora_a.default.weighttorch.Size([512, 64])torch.Size([64, 3584]) Adam-mini found the param block with name:

 torch.Size([64, 1280])base_model.model.visual.blocks.26.attn.proj.lora_b.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([1280, 64]) Adam-mini found the param block with name: 
base_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weight   base_model.model.visual.blocks.21.attn.proj.lora_b.default.weightbase_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([3584, 64])  
torch.Size([1280, 64])torch.Size([64, 3584])
Adam-mini found the param block with name:
Adam-mini found the param block with name:
  base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([512, 64])base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weighttorch.Size([64, 1280])Adam-mini found the param block with name:
 Adam-mini found the param block with name:
 base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weighttorch.Size([3584, 64])
 Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weight torch.Size([64, 1280])base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weight
Adam-mini found the param block with name:
  base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weighttorch.Size([3420, 64])  Adam-mini found the param block with name:Adam-mini found the param block with name:
torch.Size([64, 3584])torch.Size([64, 3584])  

base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3420, 64])Adam-mini found the param block with name:torch.Size([18944, 64])  
base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weight  
 torch.Size([3584, 64])base_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weighttorch.Size([18944, 64])
 
torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
 Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584]) base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weight  torch.Size([3420, 64])
 base_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([64, 3584])Adam-mini found the param block with name:

 torch.Size([3420, 64])base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([18944, 64]) base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weight
   base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weighttorch.Size([18944, 64])torch.Size([18944, 64])Adam-mini found the param block with name:  

torch.Size([64, 3420])base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: 
 torch.Size([64, 3420])base_model.model.model.layers.5.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name:   torch.Size([64, 18944])Adam-mini found the param block with name:base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weight
base_model.model.model.layers.4.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.6.mlp.down_proj.lora_a.default.weight    torch.Size([1280, 64])base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:
  torch.Size([64, 3584])torch.Size([64, 18944])torch.Size([1280, 64])base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weight


 torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.6.mlp.down_proj.lora_b.default.weighttorch.Size([18944, 64]) 
torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: torch.Size([64, 18944])base_model.model.visual.blocks.27.attn.qkv.lora_b.default.weight  base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weight
Adam-mini found the param block with name:base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weighttorch.Size([3840, 64])  
Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weight 
torch.Size([64, 1280])base_model.model.model.layers.4.mlp.down_proj.lora_b.default.weight  
torch.Size([3584, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])
Adam-mini found the param block with name:  
base_model.model.visual.blocks.27.attn.proj.lora_a.default.weight base_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weight base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 1280])  torch.Size([3584, 64])
base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weighttorch.Size([3840, 64])
Adam-mini found the param block with name:
 Adam-mini found the param block with name: torch.Size([3584, 64]) base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weightbase_model.model.visual.blocks.27.attn.proj.lora_b.default.weight
  torch.Size([64, 3584])torch.Size([1280, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:
  base_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weightbase_model.model.visual.blocks.22.attn.proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name: torch.Size([64, 3584]) torch.Size([64, 1280])base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weight
 base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weight
Adam-mini found the param block with name:torch.Size([64, 3584]) Adam-mini found the param block with name: torch.Size([3584, 64]) 
base_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name: 
base_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.visual.blocks.22.attn.proj.lora_b.default.weight  torch.Size([512, 64])base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weighttorch.Size([1280, 64]) Adam-mini found the param block with name:
Adam-mini found the param block with name:
torch.Size([512, 64])  
base_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weightbase_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([64, 3584]) 
torch.Size([3420, 64])Adam-mini found the param block with name:base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight    torch.Size([64, 3584])base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:
torch.Size([64, 3584])   
base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weighttorch.Size([512, 64]) Adam-mini found the param block with name:torch.Size([64, 1280])
Adam-mini found the param block with name: torch.Size([64, 1280])
base_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weight 
base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([512, 64])Adam-mini found the param block with name:torch.Size([512, 64]) Adam-mini found the param block with name:
 
base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weight base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weight base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64]) torch.Size([64, 3584])
torch.Size([3420, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:
  base_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weight  base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weight torch.Size([64, 3584])Adam-mini found the param block with name: torch.Size([64, 3584])
 Adam-mini found the param block with name:torch.Size([512, 64])
 
Adam-mini found the param block with name:base_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weight  base_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([64, 3420])torch.Size([64, 1280]) torch.Size([3584, 64])Adam-mini found the param block with name:
 
base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weight
base_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64]) torch.Size([64, 3584]) base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weight
 Adam-mini found the param block with name:
 base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weighttorch.Size([3420, 64])base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weight 
 Adam-mini found the param block with name:torch.Size([1280, 64]) torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weight 

 base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name:
 
 base_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name: torch.Size([64, 3420])torch.Size([18944, 64])base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name:
Adam-mini found the param block with name:   torch.Size([18944, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weight
 base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weight    base_model.model.model.layers.7.mlp.up_proj.lora_a.default.weighttorch.Size([64, 1280])torch.Size([1280, 64])torch.Size([64, 3584])
 
Adam-mini found the param block with name:
torch.Size([64, 3584]) 
base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])  base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weight
torch.Size([18944, 64])base_model.model.model.layers.7.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([18944, 64])
torch.Size([3840, 64])  

base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.23.attn.qkv.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([18944, 64])torch.Size([64, 1280]) 

base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: torch.Size([64, 3584]) base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weight
base_model.model.visual.blocks.28.attn.proj.lora_a.default.weightbase_model.model.visual.blocks.23.attn.qkv.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([64, 18944])torch.Size([3840, 64]) Adam-mini found the param block with name:base_model.model.model.layers.6.mlp.down_proj.lora_a.default.weight
torch.Size([64, 1280])
  
Adam-mini found the param block with name:base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weighttorch.Size([64, 18944])  base_model.model.model.layers.7.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:
torch.Size([18944, 64])Adam-mini found the param block with name:  
torch.Size([3584, 64])base_model.model.visual.blocks.28.attn.proj.lora_b.default.weight Adam-mini found the param block with name:
 base_model.model.visual.blocks.23.attn.proj.lora_a.default.weight torch.Size([1280, 64]) 
Adam-mini found the param block with name:base_model.model.model.layers.6.mlp.down_proj.lora_b.default.weighttorch.Size([64, 1280])  
base_model.model.model.layers.5.mlp.down_proj.lora_a.default.weighttorch.Size([3584, 64]) 
torch.Size([64, 18944])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.23.attn.proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weight 
  base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weight  torch.Size([1280, 64])Adam-mini found the param block with name: torch.Size([3584, 64])torch.Size([64, 1280])

 torch.Size([64, 3584])
base_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weight
 torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weight torch.Size([3420, 64])Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weight  
base_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weighttorch.Size([3584, 64]) base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weight 
 torch.Size([64, 1280])torch.Size([64, 3584])torch.Size([64, 3584])


Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weight base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 1280])  base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weight 
base_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weighttorch.Size([3420, 64])torch.Size([512, 64])  
torch.Size([64, 3584])Adam-mini found the param block with name:

torch.Size([3584, 64]) 
base_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3420, 64])base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weight
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:   torch.Size([512, 64])base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weight
base_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:  torch.Size([64, 3584]) torch.Size([64, 1280])torch.Size([64, 3584])

base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weight
Adam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3420])Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight
    base_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weightbase_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weight torch.Size([64, 3584])   
torch.Size([512, 64])torch.Size([3420, 64])base_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weighttorch.Size([512, 64])

Adam-mini found the param block with name: 
 torch.Size([1280, 64])base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weight
 torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 3584]) base_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weight
Adam-mini found the param block with name:base_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weight   base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3420])
  base_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weight
  torch.Size([64, 3584])base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:torch.Size([512, 64]) 
 
base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([64, 1280])torch.Size([3584, 64])Adam-mini found the param block with name:
base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weight
 Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([1280, 64])   
torch.Size([3584, 64])Adam-mini found the param block with name:base_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weightbase_model.model.visual.blocks.29.attn.qkv.lora_b.default.weight
  base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weight  torch.Size([3840, 64])torch.Size([64, 3584])torch.Size([64, 3584])


Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3584, 64]) Adam-mini found the param block with name:  base_model.model.visual.blocks.24.attn.qkv.lora_a.default.weighttorch.Size([18944, 64])
 base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weight 
 torch.Size([64, 1280])base_model.model.visual.blocks.29.attn.proj.lora_a.default.weighttorch.Size([64, 3584]) 

torch.Size([64, 1280])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weight base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.24.attn.qkv.lora_b.default.weight base_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight torch.Size([64, 3584])  torch.Size([3840, 64])
torch.Size([64, 3584])torch.Size([18944, 64])


Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.24.attn.proj.lora_a.default.weightbase_model.model.model.layers.7.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])  
base_model.model.model.layers.8.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([64, 3584]) 

torch.Size([64, 18944])Adam-mini found the param block with name:
Adam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.visual.blocks.24.attn.proj.lora_b.default.weightbase_model.model.model.layers.7.mlp.up_proj.lora_b.default.weight  base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weight base_model.model.model.layers.8.mlp.down_proj.lora_b.default.weighttorch.Size([1280, 64])  torch.Size([18944, 64])
torch.Size([18944, 64])torch.Size([3584, 64])


Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.6.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weight    torch.Size([64, 18944])torch.Size([64, 1280])torch.Size([64, 18944])base_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weight


 torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weight base_model.model.model.layers.6.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.7.mlp.down_proj.lora_b.default.weight base_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weight  torch.Size([3420, 64]) torch.Size([3584, 64])torch.Size([3584, 64])torch.Size([3584, 64])



Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: 
 base_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weight   Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([64, 3584])base_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weight
  
base_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584]) 
Adam-mini found the param block with name:torch.Size([512, 64]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weight  torch.Size([3420, 64])base_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weighttorch.Size([3584, 64]) 

Adam-mini found the param block with name:torch.Size([3584, 64]) 
base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([64, 3584]) base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weighttorch.Size([512, 64])base_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weight
 
 torch.Size([64, 3584])torch.Size([64, 3420])Adam-mini found the param block with name: 

base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weight   base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:
 torch.Size([1280, 64]) torch.Size([512, 64])Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight

  base_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weight torch.Size([64, 3584])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64]) 
base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weightbase_model.model.visual.blocks.25.attn.qkv.lora_a.default.weight    torch.Size([512, 64])torch.Size([64, 3584])
base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 1280])
 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([18944, 64]) Adam-mini found the param block with name:
base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weightbase_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weight   base_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weighttorch.Size([3840, 64])torch.Size([3584, 64]) 

torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.9.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64])  base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weight
  base_model.model.visual.blocks.25.attn.proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([18944, 64]) 

torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.proj.lora_b.default.weight base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weight base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([1280, 64])  
torch.Size([64, 3584])base_model.model.model.layers.7.mlp.up_proj.lora_a.default.weighttorch.Size([64, 18944])
 
torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight torch.Size([18944, 64]) base_model.model.model.layers.7.mlp.up_proj.lora_b.default.weight
Adam-mini found the param block with name:torch.Size([3584, 64])  
torch.Size([18944, 64])base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 1280])Adam-mini found the param block with name:
 base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 18944]) Adam-mini found the param block with name: 
base_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weight base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weight base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 3584])  
torch.Size([3420, 64])base_model.model.model.layers.7.mlp.down_proj.lora_b.default.weighttorch.Size([18944, 64])
 
Adam-mini found the param block with name:torch.Size([3584, 64]) 
base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.8.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 1280])   
base_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 18944])  
torch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name:
  base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.8.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:    torch.Size([3420, 64])torch.Size([3584, 64])base_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weight

 base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3420]) Adam-mini found the param block with name:
base_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weighttorch.Size([64, 3584])   base_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weight
 torch.Size([64, 3584])torch.Size([64, 3584])base_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weight
Adam-mini found the param block with name:
  Adam-mini found the param block with name:base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([1280, 64])   
torch.Size([512, 64])base_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weight
  torch.Size([3584, 64])torch.Size([512, 64])

Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weight   torch.Size([64, 3584])base_model.model.visual.blocks.26.attn.qkv.lora_a.default.weightbase_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weighttorch.Size([512, 64]) 
 torch.Size([64, 1280])
torch.Size([64, 3584])Adam-mini found the param block with name:
 
base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.26.attn.qkv.lora_b.default.weightbase_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weight   Adam-mini found the param block with name:torch.Size([3840, 64])torch.Size([64, 3584]) 
torch.Size([512, 64])
base_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([18944, 64])Adam-mini found the param block with name:torch.Size([3584, 64])base_model.model.visual.blocks.26.attn.proj.lora_a.default.weight 
 
base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight torch.Size([64, 1280])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weightbase_model.model.visual.blocks.26.attn.proj.lora_b.default.weightbase_model.model.model.layers.10.mlp.up_proj.lora_a.default.weight    base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([512, 64])torch.Size([1280, 64]) 

torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.model.layers.10.mlp.down_proj.lora_a.default.weight
 base_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weight base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weight  torch.Size([64, 18944])torch.Size([3584, 64])Adam-mini found the param block with name:

torch.Size([64, 3584]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weight torch.Size([3420, 64]) base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weight
torch.Size([3584, 64]) 
Adam-mini found the param block with name:torch.Size([18944, 64]) 
base_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64]) Adam-mini found the param block with name:
base_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:   base_model.model.model.layers.8.mlp.down_proj.lora_a.default.weighttorch.Size([64, 1280]) base_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weight
torch.Size([64, 18944])Adam-mini found the param block with name:
  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.9.mlp.up_proj.lora_a.default.weight 
 Adam-mini found the param block with name:base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight torch.Size([64, 3584]) base_model.model.model.layers.8.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:
torch.Size([3420, 64])  
base_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([3584, 64]) torch.Size([3584, 64])

base_model.model.model.layers.9.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weight   base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 3584]) torch.Size([64, 3584])
torch.Size([1280, 64])torch.Size([64, 18944])


Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64])Adam-mini found the param block with name: 
 base_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight  torch.Size([3584, 64])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weightbase_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weight   torch.Size([64, 1280])torch.Size([64, 3584])torch.Size([512, 64])Adam-mini found the param block with name:


 base_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weight  Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weightbase_model.model.visual.blocks.27.attn.qkv.lora_b.default.weight   torch.Size([512, 64])base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weighttorch.Size([3584, 64])
torch.Size([3840, 64]) 

torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weight base_model.model.visual.blocks.27.attn.proj.lora_a.default.weight base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weight   torch.Size([64, 3584])torch.Size([64, 1280])torch.Size([3584, 64])torch.Size([512, 64])



Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([512, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.27.attn.proj.lora_b.default.weight
  base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([1280, 64]) torch.Size([64, 3584])

base_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weight Adam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weighttorch.Size([3584, 64])Adam-mini found the param block with name:
 
 torch.Size([18944, 64])Adam-mini found the param block with name:base_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weight
  base_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weight torch.Size([64, 1280])torch.Size([512, 64])
Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weight  base_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 3584])torch.Size([64, 3584]) Adam-mini found the param block with name:

torch.Size([3420, 64]) base_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weight
 torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:
  base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([18944, 64])torch.Size([18944, 64])Adam-mini found the param block with name:base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weight

  base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weighttorch.Size([3584, 64]) 
torch.Size([64, 1280])Adam-mini found the param block with name:Adam-mini found the param block with name:
  base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.9.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 18944])torch.Size([64, 3584]) Adam-mini found the param block with name:

base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weight torch.Size([3420, 64])Adam-mini found the param block with name:base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weight 
  torch.Size([64, 3584])base_model.model.model.layers.9.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64])
 
torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 18944])Adam-mini found the param block with name: 
  base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.10.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:    torch.Size([1280, 64])torch.Size([64, 3584])torch.Size([64, 3584])base_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight


 Adam-mini found the param block with name: torch.Size([3584, 64])base_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weight
Adam-mini found the param block with name:  base_model.model.model.layers.10.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 3584])base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weightbase_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.10.mlp.down_proj.lora_a.default.weight
   torch.Size([64, 18944])torch.Size([64, 1280])torch.Size([64, 3584])Adam-mini found the param block with name:


 base_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([512, 64]) base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weight 
base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weight base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weight  torch.Size([3584, 64])torch.Size([3584, 64])torch.Size([3840, 64])


Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.28.attn.proj.lora_a.default.weightbase_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280])torch.Size([64, 3584]) 

 base_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weight  torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])
 
 base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.28.attn.proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([512, 64])base_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weight 
torch.Size([1280, 64]) base_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weight
torch.Size([3584, 64]) 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64]) Adam-mini found the param block with name:Adam-mini found the param block with name:
 base_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weight  base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584]) torch.Size([64, 1280])
torch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name: 
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.29.attn.proj.lora_b.default.weight   torch.Size([64, 3584]) torch.Size([18944, 64]) 
base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weight
 base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([512, 64])  
torch.Size([1280, 64])torch.Size([3420, 64])

base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weighttorch.Size([3584, 64]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([18944, 64])   Adam-mini found the param block with name:
torch.Size([64, 1280])base_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weight  
torch.Size([512, 64]) 
base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name: 
 torch.Size([64, 1280]) base_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weight
Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weight  Adam-mini found the param block with name:  base_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weighttorch.Size([3420, 64])base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 18944])  
base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weight 
torch.Size([18944, 64]) torch.Size([64, 3584])

torch.Size([3420, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.down_proj.lora_b.default.weight   Adam-mini found the param block with name:base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weighttorch.Size([3584, 64])   Adam-mini found the param block with name:torch.Size([64, 3420])base_model.model.model.layers.10.mlp.up_proj.lora_a.default.weight

torch.Size([3584, 64])  
base_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])  
torch.Size([64, 1280])base_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weight
 torch.Size([1280, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.10.mlp.up_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weight base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weight torch.Size([18944, 64]) base_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weight
  torch.Size([3420, 64])torch.Size([64, 3584])torch.Size([64, 3584])


Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 1280]) base_model.model.model.layers.10.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weight   
Adam-mini found the param block with name:torch.Size([64, 18944])base_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weighttorch.Size([18944, 64]) 
 base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64])   
base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weightbase_model.model.visual.blocks.29.attn.qkv.lora_b.default.weighttorch.Size([64, 3420])  
torch.Size([3584, 64])Adam-mini found the param block with name:torch.Size([3840, 64])
 Adam-mini found the param block with name:
base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:   base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])  
torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([1280, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name: 
  base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.29.attn.proj.lora_a.default.weight base_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([18944, 64])  torch.Size([64, 1280])torch.Size([64, 3584])

base_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([512, 64])  Adam-mini found the param block with name:
base_model.model.visual.blocks.30.attn.qkv.lora_a.default.weight base_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_b.default.weight   torch.Size([64, 1280])torch.Size([3584, 64])base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([1280, 64]) 

base_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weight
  torch.Size([64, 18944])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weight  base_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weighttorch.Size([3584, 64])  Adam-mini found the param block with name:torch.Size([512, 64])base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weight

torch.Size([64, 3584])  
base_model.model.visual.blocks.30.attn.proj.lora_a.default.weight torch.Size([64, 1280])torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:
 Adam-mini found the param block with name:  base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.30.attn.proj.lora_b.default.weightbase_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:    Adam-mini found the param block with name:torch.Size([3420, 64]) torch.Size([1280, 64])torch.Size([64, 3584])base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weight
  

torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weightbase_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weight   torch.Size([3584, 64])torch.Size([64, 1280])
 
torch.Size([3584, 64])torch.Size([512, 64])base_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weight

 Adam-mini found the param block with name:torch.Size([64, 1280]) 
base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 3584]) base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weight base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weight
  Adam-mini found the param block with name:torch.Size([3420, 64])torch.Size([64, 3584]) torch.Size([64, 3584])
Adam-mini found the param block with name:
base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight
  base_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3420])Adam-mini found the param block with name:Adam-mini found the param block with name:
  Adam-mini found the param block with name:torch.Size([512, 64]) base_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:base_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weight
 base_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weight   torch.Size([18944, 64])base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weighttorch.Size([3584, 64])
 torch.Size([64, 1280])

torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weight
Adam-mini found the param block with name:   torch.Size([3420, 64])base_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:
 
 base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weightbase_model.model.visual.blocks.30.attn.qkv.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([512, 64]) 
  torch.Size([64, 1280])base_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weightbase_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weighttorch.Size([18944, 64])

  torch.Size([18944, 64])Adam-mini found the param block with name:
torch.Size([64, 3420])Adam-mini found the param block with name: 
Adam-mini found the param block with name:  base_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weightbase_model.model.visual.blocks.30.attn.qkv.lora_b.default.weightbase_model.model.model.layers.13.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:    torch.Size([3840, 64])base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weightbase_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weighttorch.Size([64, 18944])torch.Size([64, 3584])


  torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([1280, 64])
  
base_model.model.model.layers.13.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([3584, 64])  

base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weightbase_model.model.visual.blocks.30.attn.proj.lora_a.default.weight  torch.Size([18944, 64])Adam-mini found the param block with name:torch.Size([64, 1280])
 
base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weight torch.Size([64, 1280])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:    base_model.model.visual.blocks.30.attn.proj.lora_b.default.weightbase_model.model.visual.blocks.31.attn.qkv.lora_b.default.weight  base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weighttorch.Size([1280, 64])torch.Size([64, 3584])  
base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weight
 torch.Size([64, 3584])torch.Size([3840, 64])Adam-mini found the param block with name:
torch.Size([64, 18944]) 
base_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name:  torch.Size([18944, 64])Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weight
base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weight   Adam-mini found the param block with name:base_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weighttorch.Size([3584, 64])torch.Size([3584, 64])  

Adam-mini found the param block with name:torch.Size([64, 1280])base_model.model.visual.blocks.31.attn.proj.lora_a.default.weight 
 base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weighttorch.Size([64, 1280]) Adam-mini found the param block with name:
torch.Size([64, 3584]) 
Adam-mini found the param block with name:base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:    Adam-mini found the param block with name:base_model.model.visual.blocks.31.attn.proj.lora_b.default.weightbase_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weighttorch.Size([3420, 64]) 
   torch.Size([1280, 64])base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584])
torch.Size([64, 3584])
 Adam-mini found the param block with name:
torch.Size([18944, 64])Adam-mini found the param block with name: 
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weight   base_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weight torch.Size([64, 1280])base_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 1280])
 
 Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([512, 64])Adam-mini found the param block with name: 

Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weight base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weight  base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weighttorch.Size([3420, 64]) torch.Size([64, 18944])Adam-mini found the param block with name:
Adam-mini found the param block with name: torch.Size([3420, 64])
 base_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weight
base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([64, 3584]) Adam-mini found the param block with name:
base_model.model.model.layers.12.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584]) 
 Adam-mini found the param block with name:base_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([3584, 64])   base_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weight
torch.Size([64, 3420])base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight
base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weight   torch.Size([512, 64])torch.Size([512, 64])
Adam-mini found the param block with name:torch.Size([64, 1280]) 

base_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3420, 64])  Adam-mini found the param block with name:
 base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weight   torch.Size([64, 3584])torch.Size([64, 3584])torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:

  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weightbase_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weight   Adam-mini found the param block with name: torch.Size([64, 3420])base_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weight base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weight
torch.Size([64, 1280]) base_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weight 
Adam-mini found the param block with name:torch.Size([3584, 64])  torch.Size([512, 64])
base_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weighttorch.Size([3584, 64])Adam-mini found the param block with name:
  
base_model.model.visual.blocks.31.attn.qkv.lora_b.default.weighttorch.Size([1280, 64]) 
torch.Size([3840, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weight  torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3584])

 Adam-mini found the param block with name:base_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.visual.blocks.31.attn.proj.lora_a.default.weight  base_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584]) torch.Size([64, 1280])torch.Size([3584, 64]) 


torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.visual.blocks.31.attn.proj.lora_b.default.weightbase_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([1280, 64]) base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weight  
 torch.Size([18944, 64]) base_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([64, 3584])
 

torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:    base_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weight base_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weight base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weight base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weight  torch.Size([18944, 64]) torch.Size([512, 64])torch.Size([3584, 64])
torch.Size([64, 3584])

torch.Size([64, 1280])

Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64]) Adam-mini found the param block with name:
base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:   base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weighttorch.Size([3420, 64])base_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:
    torch.Size([64, 3584])torch.Size([64, 3584])torch.Size([64, 3584])
base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weight

 Adam-mini found the param block with name:torch.Size([64, 18944])Adam-mini found the param block with name:Adam-mini found the param block with name:  
base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([64, 1280])  base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weight torch.Size([18944, 64])

base_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weight  torch.Size([512, 64])torch.Size([3584, 64])Adam-mini found the param block with name:
 torch.Size([3584, 64])
Adam-mini found the param block with name:base_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weight
  base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weighttorch.Size([3420, 64]) 
torch.Size([64, 18944])Adam-mini found the param block with name:
 base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.12.mlp.down_proj.lora_b.default.weight   base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([3584, 64]) torch.Size([512, 64])
base_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weight 

torch.Size([64, 3420])
 Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name: 
 torch.Size([3584, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weight
 Adam-mini found the param block with name:base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weight    base_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weighttorch.Size([1280, 64])torch.Size([64, 3584])  
Adam-mini found the param block with name:
torch.Size([18944, 64])torch.Size([64, 3584]) 

Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584]) 

Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight torch.Size([3584, 64])base_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weight 
 Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([512, 64])
 
base_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight
base_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weighttorch.Size([18944, 64])  base_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weight  
torch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([512, 64])


 base_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name: 
 Adam-mini found the param block with name:base_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weight base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weight base_model.model.model.layers.13.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([512, 64])   base_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weighttorch.Size([18944, 64])
Adam-mini found the param block with name: torch.Size([64, 18944])

 torch.Size([64, 3584])base_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name:  torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.13.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:
    base_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.0.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([3584, 64]) torch.Size([64, 3584])torch.Size([64, 3584])
base_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weighttorch.Size([512, 64])

 
torch.Size([512, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight  base_model.model.model.layers.0.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64]) 
Adam-mini found the param block with name:torch.Size([18944, 64]) 
base_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weight
base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weight  Adam-mini found the param block with name: torch.Size([64, 3584])Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])
 base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weight 
Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weight  Adam-mini found the param block with name: 
torch.Size([64, 18944])torch.Size([3584, 64])base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weight
 
 Adam-mini found the param block with name:base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weight torch.Size([3584, 64]) Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight
torch.Size([512, 64])base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weight  Adam-mini found the param block with name:
torch.Size([3584, 64])torch.Size([18944, 64]) 

base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
Adam-mini found the param block with name:base_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weighttorch.Size([18944, 64]) base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: 
base_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weight  torch.Size([64, 3584])base_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
 torch.Size([512, 64])
torch.Size([64, 3584])Adam-mini found the param block with name:

 Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:   base_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name: 
 base_model.model.model.layers.15.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64])  base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([18944, 64])
 torch.Size([3584, 64]) 
torch.Size([64, 3584])

base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.15.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 18944])  Adam-mini found the param block with name:base_model.model.model.layers.13.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3584])
base_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weight   
base_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 18944])Adam-mini found the param block with name:torch.Size([64, 3584]) 
 
Adam-mini found the param block with name:base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584]) 
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([3584, 64]) base_model.model.model.layers.13.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weight 
torch.Size([18944, 64])base_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weight  
 torch.Size([3584, 64])torch.Size([512, 64])torch.Size([3584, 64])


Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.0.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: 
 torch.Size([64, 3584]) base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weight
Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weight   torch.Size([64, 3584]) torch.Size([64, 3584])base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])

  torch.Size([3584, 64])
base_model.model.model.layers.0.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:
  Adam-mini found the param block with name:base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weighttorch.Size([18944, 64])Adam-mini found the param block with name:  
 torch.Size([3584, 64])base_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:
   torch.Size([512, 64])torch.Size([18944, 64])
base_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weight
Adam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([64, 3584])base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weight
 Adam-mini found the param block with name:  torch.Size([64, 18944]) base_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weight
base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 3584]) 
  base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3584])  
 
base_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weighttorch.Size([512, 64])base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([512, 64])
Adam-mini found the param block with name: 
base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64])  
base_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weighttorch.Size([18944, 64]) 
Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name: 
base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weight  base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weighttorch.Size([64, 3584]) 
Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weighttorch.Size([64, 18944])Adam-mini found the param block with name: 
Adam-mini found the param block with name:Adam-mini found the param block with name:  torch.Size([512, 64])base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:
   torch.Size([512, 64])base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])
  
Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584]) Adam-mini found the param block with name:

Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight base_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weight torch.Size([64, 3584])  base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 3584])
torch.Size([3584, 64]) 

torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([3584, 64])  Adam-mini found the param block with name:torch.Size([3584, 64])
torch.Size([64, 3584])base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weight 

base_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weight  torch.Size([64, 3584])
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64]) Adam-mini found the param block with name:  
base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weight    torch.Size([18944, 64])base_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([512, 64])

 Adam-mini found the param block with name:
 torch.Size([64, 3584])base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weight
Adam-mini found the param block with name:  base_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:
torch.Size([18944, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name:   base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weight   Adam-mini found the param block with name: torch.Size([512, 64])torch.Size([18944, 64])
torch.Size([64, 18944]) torch.Size([64, 3584])base_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight
 

torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:    base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.1.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.16.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weight     torch.Size([64, 3584])torch.Size([3584, 64])torch.Size([18944, 64])
torch.Size([512, 64])
torch.Size([64, 3584])

Adam-mini found the param block with name:
 base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.16.mlp.down_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([512, 64])torch.Size([64, 18944])Adam-mini found the param block with name:
 
 base_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weight    base_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weight  
torch.Size([64, 18944])torch.Size([3584, 64])torch.Size([64, 3584])base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weight


Adam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weight torch.Size([64, 3584])  base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weight
torch.Size([3584, 64])base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weight
  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([3584, 64]) 
 base_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight
base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight  torch.Size([3584, 64])torch.Size([64, 3584])Adam-mini found the param block with name:

 base_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weight Adam-mini found the param block with name:  base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weight  base_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weighttorch.Size([3584, 64])Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weight 
 
 torch.Size([64, 3584])base_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weighttorch.Size([18944, 64])
Adam-mini found the param block with name:
  base_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: Adam-mini found the param block with name:
 torch.Size([512, 64]) base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584])  
torch.Size([3584, 64]) base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weight    base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([18944, 64])base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weight 
  
torch.Size([512, 64])torch.Size([64, 3584])Adam-mini found the param block with name:
base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weight
  base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:Adam-mini found the param block with name:
torch.Size([18944, 64])  Adam-mini found the param block with name:
base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:   base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weight torch.Size([64, 3584])  torch.Size([512, 64])
torch.Size([512, 64])torch.Size([64, 3584])Adam-mini found the param block with name:

 
Adam-mini found the param block with name:base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:   base_model.model.model.layers.15.mlp.up_proj.lora_b.default.weighttorch.Size([64, 18944])base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:
Adam-mini found the param block with name:    torch.Size([18944, 64])torch.Size([512, 64])base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weight
  
 torch.Size([64, 3584])base_model.model.model.layers.1.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584])
 
torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:    base_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.15.mlp.down_proj.lora_a.default.weight    torch.Size([3584, 64])torch.Size([512, 64])torch.Size([64, 18944])torch.Size([64, 3584])Adam-mini found the param block with name:



 base_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: 
 base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([3584, 64])Adam-mini found the param block with name:
  base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weight torch.Size([3584, 64])  base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weight
torch.Size([64, 3584])torch.Size([64, 3584]) 

torch.Size([3584, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight   base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([3584, 64])   
Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight torch.Size([18944, 64])
 
base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584]) 
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])  Adam-mini found the param block with name:
base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:   base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weighttorch.Size([3584, 64]) torch.Size([64, 3584]) 
base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weight  
torch.Size([18944, 64])torch.Size([64, 3584])torch.Size([512, 64])


Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64])Adam-mini found the param block with name: 
base_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: 
 base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weight torch.Size([18944, 64])Adam-mini found the param block with name: Adam-mini found the param block with name:
base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])  base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weight
torch.Size([64, 3584])base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weight 
Adam-mini found the param block with name: torch.Size([64, 3584])torch.Size([512, 64]) 

base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:  torch.Size([18944, 64])base_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.15.mlp.up_proj.lora_b.default.weight
  torch.Size([512, 64])Adam-mini found the param block with name:torch.Size([18944, 64]) 
base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weight
 torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64]) Adam-mini found the param block with name:base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: 
  base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weighttorch.Size([3584, 64])base_model.model.model.layers.15.mlp.down_proj.lora_a.default.weight 
 torch.Size([64, 3584])torch.Size([64, 18944])

Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: 
base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weight  base_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64])   
base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weighttorch.Size([3584, 64])base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weight  
torch.Size([64, 3584])torch.Size([3584, 64])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.2.mlp.down_proj.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 18944]) 
base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3584, 64])  torch.Size([64, 3584])Adam-mini found the param block with name:
base_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weight base_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weight
base_model.model.model.layers.2.mlp.down_proj.lora_b.default.weight   Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([64, 3584])torch.Size([3584, 64]) Adam-mini found the param block with name:

 base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weight
base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3584, 64]) 
 torch.Size([64, 3584])base_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weight
  torch.Size([18944, 64])torch.Size([18944, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:

  base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([512, 64])torch.Size([64, 3584])
Adam-mini found the param block with name:
 base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:
 base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584])torch.Size([64, 3584])
  
Adam-mini found the param block with name:torch.Size([512, 64])base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:  
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weight base_model.model.model.layers.16.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584]) base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weight 
torch.Size([3584, 64]) Adam-mini found the param block with name:torch.Size([18944, 64])
torch.Size([18944, 64])
 Adam-mini found the param block with name:
base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weight  base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weight torch.Size([64, 3584])torch.Size([512, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.16.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weight base_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 18944])torch.Size([512, 64])base_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 3584])

 
torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3584, 64])  base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weight
base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight   torch.Size([512, 64])torch.Size([3584, 64])torch.Size([64, 3584])


Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name: 
 base_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:   Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584]) 
 
base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584]) 
Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.2.mlp.down_proj.lora_a.default.weight   Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weighttorch.Size([512, 64]) torch.Size([18944, 64])base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight
  torch.Size([64, 18944])
torch.Size([64, 3584])torch.Size([18944, 64])


Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.model.layers.2.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight   torch.Size([512, 64])torch.Size([3584, 64]) base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weight

torch.Size([64, 3584])torch.Size([64, 3584]) 
torch.Size([18944, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.16.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:  base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weight torch.Size([3584, 64])torch.Size([18944, 64]) base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weight
Adam-mini found the param block with name:
torch.Size([64, 3584])  
torch.Size([64, 18944])base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: Adam-mini found the param block with name:
 Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_a.default.weight base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.18.mlp.down_proj.lora_b.default.weight base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:
   torch.Size([512, 64])torch.Size([3584, 64])torch.Size([64, 3584])Adam-mini found the param block with name:
base_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weight
 
 base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64]) 
base_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: 
 Adam-mini found the param block with name:torch.Size([64, 3584]) base_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight
 torch.Size([64, 3584])  base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:
torch.Size([64, 3584])base_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weight 
  Adam-mini found the param block with name:base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([3584, 64])  
 base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weighttorch.Size([3584, 64])
base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([512, 64]) torch.Size([3584, 64])base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weight

 Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([18944, 64])  
base_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([64, 3584]) Adam-mini found the param block with name:
base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight
  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584]) Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weight
 torch.Size([64, 3584]) 
 base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 18944])torch.Size([512, 64]) 
 torch.Size([18944, 64])Adam-mini found the param block with name:
base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weight
  base_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weighttorch.Size([512, 64])Adam-mini found the param block with name: Adam-mini found the param block with name:  
torch.Size([512, 64])base_model.model.model.layers.3.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weight
Adam-mini found the param block with name:   torch.Size([3584, 64])torch.Size([64, 3584])base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weight

Adam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:
   torch.Size([64, 3584])base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weight
   base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64])Adam-mini found the param block with name:torch.Size([64, 3584]) 
torch.Size([18944, 64]) base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weight

base_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([512, 64])Adam-mini found the param block with name:torch.Size([64, 3584]) 
 
Adam-mini found the param block with name:base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 3584]) base_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([64, 18944])
 
base_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weighttorch.Size([3584, 64])Adam-mini found the param block with name: 
 torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weight
  base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weighttorch.Size([3584, 64]) Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name:

 Adam-mini found the param block with name:base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weight   base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weight torch.Size([64, 3584]) torch.Size([64, 3584])
torch.Size([3584, 64])
Adam-mini found the param block with name:
 base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])   Adam-mini found the param block with name:torch.Size([18944, 64])
base_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weight
base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weight  base_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([64, 3584])torch.Size([512, 64]) Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight
 
 torch.Size([18944, 64])
base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([18944, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64]) 
  
base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight   torch.Size([64, 3584])torch.Size([18944, 64])torch.Size([64, 3584])Adam-mini found the param block with name:
 

base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight 
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 3584])  
torch.Size([512, 64])torch.Size([18944, 64])base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.3.mlp.down_proj.lora_a.default.weight
  
Adam-mini found the param block with name:torch.Size([18944, 64]) torch.Size([64, 18944])
base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weight
 torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:    base_model.model.model.layers.19.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.3.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584]) base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: 
torch.Size([64, 18944])torch.Size([3584, 64])  
torch.Size([64, 18944])
base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name: 
 torch.Size([64, 3584])base_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:
  torch.Size([3584, 64])base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:
   torch.Size([3584, 64])base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.19.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:
  torch.Size([512, 64]) torch.Size([3584, 64])

base_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584]) 
torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:  base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:     torch.Size([64, 3584])torch.Size([18944, 64])torch.Size([3584, 64])base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weight

 torch.Size([64, 3584])
torch.Size([64, 3584])
Adam-mini found the param block with name:
 base_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:    base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.4.mlp.up_proj.lora_a.default.weighttorch.Size([3584, 64])   torch.Size([3584, 64])Adam-mini found the param block with name:torch.Size([64, 3584])

torch.Size([64, 3584]) 

base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weight   base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weighttorch.Size([512, 64])  base_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weight
torch.Size([18944, 64])base_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weighttorch.Size([512, 64]) 
 
torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weight base_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.4.mlp.down_proj.lora_a.default.weight  torch.Size([64, 3584])  
 torch.Size([64, 18944])base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weighttorch.Size([18944, 64])
torch.Size([512, 64]) Adam-mini found the param block with name:

torch.Size([64, 3584])Adam-mini found the param block with name: 
 base_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.4.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weighttorch.Size([512, 64]) 
torch.Size([3584, 64])base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weight base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weighttorch.Size([512, 64]) 
 
torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weight   base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weight base_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:torch.Size([18944, 64]) 
torch.Size([64, 3584])torch.Size([512, 64])
 

base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weight 
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weighttorch.Size([3584, 64])torch.Size([3584, 64])  Adam-mini found the param block with name:
torch.Size([64, 18944])base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weight
 
 base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([3584, 64])Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.18.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weighttorch.Size([3584, 64])Adam-mini found the param block with name:
   Adam-mini found the param block with name:base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weighttorch.Size([3584, 64])base_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weight   base_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weight
torch.Size([64, 3584])torch.Size([64, 3584])
 
torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weight base_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:  base_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weight  torch.Size([18944, 64])base_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weighttorch.Size([18944, 64]) torch.Size([512, 64])
 
torch.Size([64, 3584])torch.Size([64, 3584])


Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.4.mlp.up_proj.lora_a.default.weight   torch.Size([18944, 64])
torch.Size([3584, 64])torch.Size([64, 3584])base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weight


 Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([18944, 64])base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:   
base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weighttorch.Size([18944, 64]) base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weight base_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weight
  torch.Size([512, 64])Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([64, 3584])
 

base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 18944])Adam-mini found the param block with name:Adam-mini found the param block with name: 
  Adam-mini found the param block with name:base_model.model.model.layers.4.mlp.down_proj.lora_a.default.weight base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weight   torch.Size([18944, 64]) torch.Size([64, 18944])torch.Size([512, 64])
torch.Size([64, 3584])base_model.model.model.layers.18.mlp.down_proj.lora_b.default.weight


 torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.4.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weight   base_model.model.model.layers.20.mlp.down_proj.lora_a.default.weighttorch.Size([3584, 64])torch.Size([3584, 64])Adam-mini found the param block with name: 

 torch.Size([64, 18944])Adam-mini found the param block with name:base_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weight
  base_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 3584]) 
Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weight base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weight torch.Size([3584, 64]) Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weight
torch.Size([512, 64])  
base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weight
torch.Size([3584, 64]) 
Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weighttorch.Size([3584, 64]) base_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:
 torch.Size([64, 3584])base_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weight 
base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weighttorch.Size([64, 3584])  
torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:
   Adam-mini found the param block with name:base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weight   base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weight  base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weighttorch.Size([512, 64])torch.Size([3584, 64]) 

 torch.Size([64, 3584])torch.Size([18944, 64])
torch.Size([3584, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([512, 64]) 
base_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weight
base_model.model.model.layers.5.mlp.down_proj.lora_a.default.weight   torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3584]) torch.Size([64, 18944])
Adam-mini found the param block with name:
base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weight
 base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:  torch.Size([512, 64]) Adam-mini found the param block with name:
 torch.Size([64, 3584])base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight 
  torch.Size([3584, 64])
torch.Size([512, 64])Adam-mini found the param block with name:torch.Size([18944, 64])Adam-mini found the param block with name: 
base_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weight 
 base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([3584, 64]) Adam-mini found the param block with name:base_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight 
 base_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 3584])  base_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weight
torch.Size([64, 3584])torch.Size([64, 3584]) 

Adam-mini found the param block with name: torch.Size([512, 64])Adam-mini found the param block with name:base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weight
 Adam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weight  base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weight 
Adam-mini found the param block with name:torch.Size([3584, 64]) torch.Size([3584, 64]) 
base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([18944, 64])
 
torch.Size([64, 3584])base_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight
 torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name:Adam-mini found the param block with name: 
  base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.19.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weight    torch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 18944])base_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight

 
 torch.Size([64, 3584])base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name: 
Adam-mini found the param block with name: torch.Size([64, 3584]) base_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weight
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weight  torch.Size([512, 64])base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([18944, 64])
  base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weight  base_model.model.model.layers.19.mlp.down_proj.lora_b.default.weight
torch.Size([18944, 64]) torch.Size([18944, 64])

torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weight  base_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.19.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.21.mlp.up_proj.lora_a.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name: torch.Size([512, 64]) 
 torch.Size([64, 18944])
base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 3584])
 Adam-mini found the param block with name:
torch.Size([64, 3584]) 
base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([18944, 64])Adam-mini found the param block with name:base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weight 
  Adam-mini found the param block with name:base_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weighttorch.Size([18944, 64])   
base_model.model.model.layers.19.mlp.down_proj.lora_b.default.weighttorch.Size([3584, 64])torch.Size([64, 3584]) Adam-mini found the param block with name:

torch.Size([3584, 64]) 
base_model.model.model.layers.5.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 18944]) base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weight
base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([64, 18944]) torch.Size([3584, 64])Adam-mini found the param block with name:base_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weight

Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weight  
 torch.Size([3584, 64])base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: 

Adam-mini found the param block with name: torch.Size([3584, 64]) base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:
base_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weight   torch.Size([64, 3584])base_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weighttorch.Size([512, 64])
 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weighttorch.Size([18944, 64])  
base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])  
 torch.Size([64, 3584])base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])    base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584]) 
base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weight  
torch.Size([3584, 64])torch.Size([64, 3584])torch.Size([3584, 64])
Adam-mini found the param block with name:

Adam-mini found the param block with name:  base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([512, 64]) 
torch.Size([512, 64])Adam-mini found the param block with name:base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:
   base_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weighttorch.Size([18944, 64]) 
 Adam-mini found the param block with name:torch.Size([64, 3584]) Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weight  

base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: 
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])   Adam-mini found the param block with name:base_model.model.model.layers.6.mlp.down_proj.lora_a.default.weight
 base_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weight base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weight torch.Size([64, 18944])Adam-mini found the param block with name: torch.Size([512, 64]) torch.Size([512, 64])
torch.Size([3584, 64]) 

base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weight
Adam-mini found the param block with name:  torch.Size([512, 64])base_model.model.model.layers.6.mlp.down_proj.lora_b.default.weight
 Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name: 
 base_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])   base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 3584]) torch.Size([64, 3584])Adam-mini found the param block with name:

torch.Size([64, 3584]) Adam-mini found the param block with name:
base_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:    torch.Size([512, 64])base_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weight base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weight
 base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight  torch.Size([64, 3584])torch.Size([512, 64]) torch.Size([3584, 64])

torch.Size([18944, 64])
Adam-mini found the param block with name:
 base_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:
torch.Size([3584, 64])Adam-mini found the param block with name:Adam-mini found the param block with name: 
  Adam-mini found the param block with name:base_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weight base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weight   base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weighttorch.Size([64, 3584])torch.Size([64, 3584]) torch.Size([64, 3584])

torch.Size([3584, 64])Adam-mini found the param block with name:

 Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight torch.Size([64, 3584])base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weight base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weight
Adam-mini found the param block with name:torch.Size([18944, 64])   Adam-mini found the param block with name:torch.Size([3584, 64])
base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weighttorch.Size([18944, 64]) 
 
Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584])  
base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weighttorch.Size([512, 64]) 
torch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.20.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:torch.Size([18944, 64]) torch.Size([64, 18944]) Adam-mini found the param block with name:
base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weight
base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weight  base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])  torch.Size([18944, 64])torch.Size([64, 3584])
Adam-mini found the param block with name:

base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([3584, 64])  base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name:  base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.model.layers.20.mlp.down_proj.lora_a.default.weighttorch.Size([18944, 64]) 
 
torch.Size([512, 64])torch.Size([64, 18944])
Adam-mini found the param block with name:
 base_model.model.model.layers.22.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:   Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weighttorch.Size([3584, 64])
Adam-mini found the param block with name:  
 base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([64, 3584]) base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 3584])
base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weight 
 torch.Size([64, 18944])Adam-mini found the param block with name:torch.Size([3584, 64])

Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weight  base_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weight base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight torch.Size([18944, 64])torch.Size([64, 3584])Adam-mini found the param block with name: torch.Size([3584, 64])


 torch.Size([3584, 64])base_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:
  base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.6.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weight torch.Size([512, 64])base_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 18944])
 torch.Size([64, 3584])
torch.Size([64, 3584])


Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight   Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.6.mlp.down_proj.lora_b.default.weighttorch.Size([18944, 64]) torch.Size([3584, 64])  
base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weighttorch.Size([3584, 64])
torch.Size([64, 3584])
 
torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.7.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weight    base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([512, 64])torch.Size([512, 64]) 
Adam-mini found the param block with name:

torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weight
  base_model.model.model.layers.7.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:
torch.Size([18944, 64])Adam-mini found the param block with name: Adam-mini found the param block with name:
 base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weight  base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weight
  Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([3584, 64])torch.Size([512, 64])Adam-mini found the param block with name:
 

 base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([64, 18944])
torch.Size([3584, 64])base_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: 
 torch.Size([3584, 64]) Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weight 
  torch.Size([64, 3584])base_model.model.model.layers.7.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584])
 
Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name: 
base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584])   
base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weighttorch.Size([512, 64]) 
base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weighttorch.Size([64, 3584]) 
Adam-mini found the param block with name:torch.Size([512, 64]) Adam-mini found the param block with name:
base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:   base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weighttorch.Size([18944, 64])   base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight
torch.Size([64, 3584])torch.Size([18944, 64])Adam-mini found the param block with name: 

 torch.Size([64, 3584])base_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 3584])Adam-mini found the param block with name: 
Adam-mini found the param block with name:base_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weight base_model.model.model.layers.21.mlp.up_proj.lora_a.default.weight   base_model.model.model.layers.21.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([64, 3584])torch.Size([64, 3584])  

base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weighttorch.Size([512, 64]) 
Adam-mini found the param block with name:torch.Size([3584, 64]) Adam-mini found the param block with name:
Adam-mini found the param block with name:base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weight   base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weighttorch.Size([18944, 64])Adam-mini found the param block with name:  
Adam-mini found the param block with name: torch.Size([18944, 64])torch.Size([64, 3584])
 base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weight
base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:
 
 base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([64, 18944])Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weight  
 torch.Size([512, 64])base_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weighttorch.Size([64, 18944])base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weight
 Adam-mini found the param block with name:
 torch.Size([3584, 64]) 
base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weighttorch.Size([18944, 64])Adam-mini found the param block with name: 
 torch.Size([3584, 64])base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:
  torch.Size([3584, 64])base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:
   base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584]) torch.Size([64, 3584])
torch.Size([64, 3584])Adam-mini found the param block with name:

 Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:    base_model.model.model.layers.23.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584])base_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weight
base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight   torch.Size([64, 3584])torch.Size([18944, 64]) Adam-mini found the param block with name:torch.Size([18944, 64])

 
torch.Size([512, 64])base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:
  base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weighttorch.Size([3584, 64]) 
Adam-mini found the param block with name:torch.Size([3584, 64]) 
Adam-mini found the param block with name:base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([64, 18944])base_model.model.model.layers.7.mlp.up_proj.lora_a.default.weight  base_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weight
Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name:   
 torch.Size([64, 3584])base_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight
Adam-mini found the param block with name:    torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584])base_model.model.model.layers.7.mlp.up_proj.lora_b.default.weight

 
 base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weight torch.Size([18944, 64])torch.Size([3584, 64])Adam-mini found the param block with name:Adam-mini found the param block with name:

  base_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weight  torch.Size([512, 64])torch.Size([512, 64])

Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])
 base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:  torch.Size([64, 18944])base_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weight base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weight
  base_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])torch.Size([64, 3584])
Adam-mini found the param block with name:
torch.Size([3584, 64]) 

Adam-mini found the param block with name:base_model.model.model.layers.7.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weighttorch.Size([3584, 64])   Adam-mini found the param block with name:torch.Size([18944, 64])torch.Size([512, 64])
base_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weight 

base_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight  torch.Size([512, 64])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([512, 64])  Adam-mini found the param block with name:
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weight base_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weight  base_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name: torch.Size([64, 3584])torch.Size([64, 3584])
 
torch.Size([64, 3584])
base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weight
 Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: 
Adam-mini found the param block with name:  base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weight base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3584, 64]) base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weight 
 torch.Size([18944, 64]) torch.Size([3584, 64]) 

torch.Size([512, 64])torch.Size([3584, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.8.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:   base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 18944]) 
torch.Size([64, 3584])  
base_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:
 Adam-mini found the param block with name: torch.Size([64, 3584])
base_model.model.model.layers.8.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: 
 Adam-mini found the param block with name:torch.Size([3584, 64])base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weight 
 base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:base_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weighttorch.Size([18944, 64])   
base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weighttorch.Size([3584, 64])torch.Size([18944, 64]) 

torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 3584])
Adam-mini found the param block with name:base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weight 
base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weight   Adam-mini found the param block with name:base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name: torch.Size([64, 3584])torch.Size([64, 3584])  
torch.Size([64, 3584])base_model.model.model.layers.22.mlp.up_proj.lora_b.default.weight

base_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([18944, 64])torch.Size([3584, 64]) 
 
 base_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.22.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weight   Adam-mini found the param block with name:torch.Size([18944, 64])torch.Size([512, 64])torch.Size([18944, 64]) 


Adam-mini found the param block with name:base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight  base_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weight torch.Size([64, 18944])torch.Size([64, 3584])
Adam-mini found the param block with name:
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight base_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.24.mlp.up_proj.lora_a.default.weight  base_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weight  torch.Size([3584, 64])torch.Size([512, 64])torch.Size([64, 3584])torch.Size([64, 18944]) 



torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64]) Adam-mini found the param block with name:
base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([3584, 64])base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584]) 
 

base_model.model.model.layers.24.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weight  torch.Size([64, 18944])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.24.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3584, 64]) Adam-mini found the param block with name:
 base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weight torch.Size([3584, 64])Adam-mini found the param block with name: 
base_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584]) base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weight
Adam-mini found the param block with name: torch.Size([64, 3584]) 
torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weight
  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:
Adam-mini found the param block with name:   base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weight base_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weight torch.Size([18944, 64])  torch.Size([64, 3584])torch.Size([3584, 64])base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weight

torch.Size([3584, 64])
 
Adam-mini found the param block with name:torch.Size([512, 64]) 
base_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 3584]) base_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weight
torch.Size([64, 3584])base_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weight
   Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584])

  base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:
torch.Size([18944, 64])  Adam-mini found the param block with name:
base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([512, 64])   
torch.Size([512, 64])base_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weight
  torch.Size([512, 64])torch.Size([18944, 64])Adam-mini found the param block with name:

 base_model.model.model.layers.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weight  torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584]) 

 base_model.model.model.layers.8.mlp.down_proj.lora_b.default.weight base_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weight base_model.model.model.layers.9.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([3584, 64]) torch.Size([64, 3584]) base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weight 

 torch.Size([64, 3584])base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weight
torch.Size([3584, 64]) 
torch.Size([512, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name:  base_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.9.mlp.up_proj.lora_b.default.weight  torch.Size([512, 64])torch.Size([18944, 64])Adam-mini found the param block with name:

 Adam-mini found the param block with name:base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:   base_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 3584])  Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name:
 
torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weight
base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:    base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])torch.Size([3584, 64])torch.Size([64, 18944])base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weight
  

torch.Size([18944, 64])torch.Size([3584, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight  Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name:torch.Size([3584, 64])
Adam-mini found the param block with name:  
 base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight   torch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3584])

 
base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.23.mlp.up_proj.lora_b.default.weight torch.Size([64, 3584]) 
base_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weight torch.Size([64, 3584])torch.Size([18944, 64])base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:

torch.Size([512, 64])torch.Size([18944, 64]) 
Adam-mini found the param block with name:
base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weight  base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([18944, 64])  
base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weighttorch.Size([3584, 64])Adam-mini found the param block with name: 
Adam-mini found the param block with name: torch.Size([64, 18944]) base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight
Adam-mini found the param block with name:base_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight   Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([64, 3584]) torch.Size([64, 3584])base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weight
  base_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight
base_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])torch.Size([3584, 64])  base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name:

 
 torch.Size([512, 64])base_model.model.model.layers.23.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:
 Adam-mini found the param block with name: torch.Size([18944, 64]) base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight
base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:   torch.Size([512, 64])base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weighttorch.Size([18944, 64])Adam-mini found the param block with name:
  
Adam-mini found the param block with name:base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weighttorch.Size([64, 3584])  
base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 3584])torch.Size([64, 18944])Adam-mini found the param block with name:
Adam-mini found the param block with name:Adam-mini found the param block with name:
   base_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([3584, 64])  base_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight
torch.Size([64, 18944])torch.Size([64, 3584])  

torch.Size([3584, 64])torch.Size([3584, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.25.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight   base_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weighttorch.Size([3584, 64])torch.Size([64, 3584])Adam-mini found the param block with name:
 
 Adam-mini found the param block with name:torch.Size([512, 64])base_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weight 
Adam-mini found the param block with name:base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weight   base_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584])torch.Size([64, 3584]) 

torch.Size([512, 64])Adam-mini found the param block with name:
Adam-mini found the param block with name: Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weight  base_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weight 
torch.Size([64, 3584])torch.Size([3584, 64])  

Adam-mini found the param block with name:base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weighttorch.Size([18944, 64])  Adam-mini found the param block with name:
base_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 3584]) 
 Adam-mini found the param block with name:base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight
  Adam-mini found the param block with name:torch.Size([3584, 64]) torch.Size([64, 3584])base_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weight
base_model.model.model.layers.9.mlp.up_proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([512, 64]) 
torch.Size([64, 3584])base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:
  base_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name: Adam-mini found the param block with name:
torch.Size([512, 64])Adam-mini found the param block with name: 
 Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weight   base_model.model.model.layers.9.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:torch.Size([64, 3584]) 
 torch.Size([18944, 64])
base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weighttorch.Size([512, 64])
 Adam-mini found the param block with name:Adam-mini found the param block with name:
 torch.Size([64, 3584]) base_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weight
base_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name:Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([18944, 64])Adam-mini found the param block with name:  base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weight

 base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weight   torch.Size([64, 18944])torch.Size([64, 3584])torch.Size([512, 64])


Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:base_model.model.model.layers.10.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight 
 base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weight  base_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584])torch.Size([512, 64])  


base_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([18944, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.model.layers.10.mlp.up_proj.lora_b.default.weighttorch.Size([3584, 64])base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weight 
 base_model.model.model.layers.24.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([18944, 64])torch.Size([64, 3584]) torch.Size([64, 3584])

base_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weight
 Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: 
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weight base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weight base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([18944, 64])torch.Size([64, 3584])torch.Size([3584, 64])
base_model.model.model.layers.10.mlp.down_proj.lora_a.default.weight 

base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weight  torch.Size([64, 18944])torch.Size([3584, 64])
Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:torch.Size([18944, 64])   
base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weighttorch.Size([64, 18944])base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weight
Adam-mini found the param block with name:   torch.Size([3584, 64])Adam-mini found the param block with name:base_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])
  Adam-mini found the param block with name:
base_model.model.model.layers.24.mlp.down_proj.lora_b.default.weight  torch.Size([64, 3584])base_model.model.model.layers.24.mlp.up_proj.lora_a.default.weighttorch.Size([3584, 64])
Adam-mini found the param block with name:  
base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584]) 
 torch.Size([18944, 64])base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weight
Adam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([512, 64])base_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weight 
 base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 3584])Adam-mini found the param block with name: torch.Size([18944, 64]) 
base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weight
 base_model.model.model.layers.26.mlp.up_proj.lora_a.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:Adam-mini found the param block with name:
torch.Size([64, 3584])  
base_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:  torch.Size([3584, 64])torch.Size([64, 3584])
Adam-mini found the param block with name:base_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.24.mlp.down_proj.lora_a.default.weight 
 torch.Size([3584, 64])base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weight
  Adam-mini found the param block with name:torch.Size([18944, 64])torch.Size([64, 18944]) 
Adam-mini found the param block with name:
base_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([512, 64])  base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3584]) base_model.model.model.layers.24.mlp.down_proj.lora_b.default.weight
 torch.Size([64, 3584])
torch.Size([64, 18944])torch.Size([3584, 64])


Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weight    base_model.model.model.layers.26.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weighttorch.Size([512, 64]) torch.Size([512, 64]) Adam-mini found the param block with name:
torch.Size([3584, 64])torch.Size([64, 3584])
 
base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 3584])Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weight
  base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weight  Adam-mini found the param block with name:torch.Size([3584, 64])Adam-mini found the param block with name:torch.Size([3584, 64])
torch.Size([64, 3584])  

base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weight  torch.Size([64, 3584])torch.Size([512, 64])Adam-mini found the param block with name:

 base_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   Adam-mini found the param block with name:base_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weight    Adam-mini found the param block with name:base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weighttorch.Size([3584, 64])torch.Size([64, 3584])torch.Size([64, 3584])
  
torch.Size([64, 3584])
base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weight
 Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weight
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([18944, 64])base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weight
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weight base_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weight  torch.Size([3584, 64])base_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weight
 torch.Size([64, 3584])Adam-mini found the param block with name:
 torch.Size([512, 64]) torch.Size([3584, 64])
base_model.model.model.layers.10.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:
  Adam-mini found the param block with name:torch.Size([64, 3584])base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight 
 base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weight torch.Size([512, 64])torch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name: 
 Adam-mini found the param block with name:base_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.10.mlp.up_proj.lora_b.default.weight   Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weighttorch.Size([18944, 64])torch.Size([64, 3584])base_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weight
base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weight  
 torch.Size([64, 3584])torch.Size([64, 3584])torch.Size([18944, 64])


Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:Adam-mini found the param block with name:   torch.Size([512, 64])Adam-mini found the param block with name:base_model.model.model.layers.10.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight
   torch.Size([512, 64])torch.Size([64, 18944])base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weight
 
 torch.Size([64, 3584])torch.Size([18944, 64])Adam-mini found the param block with name:

 Adam-mini found the param block with name:base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight
  base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weighttorch.Size([18944, 64]) 

 base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weight torch.Size([64, 3584]) torch.Size([64, 3584])
torch.Size([3584, 64])
Adam-mini found the param block with name:
 Adam-mini found the param block with name:base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weight  base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name: torch.Size([64, 18944]) torch.Size([3584, 64])
base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weight
Adam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([18944, 64])base_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:
   base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.model.layers.25.mlp.down_proj.lora_b.default.weight
torch.Size([64, 3584])  base_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weighttorch.Size([3584, 64])
 Adam-mini found the param block with name:Adam-mini found the param block with name: 
 torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weight
base_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weight   base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 18944])torch.Size([3584, 64])Adam-mini found the param block with name:
 
 torch.Size([18944, 64])base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name: 
 Adam-mini found the param block with name:torch.Size([18944, 64])base_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weight 
 base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([3584, 64])torch.Size([64, 3584]) 
Adam-mini found the param block with name:
base_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weight  torch.Size([64, 3584]) base_model.model.model.layers.27.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weight
torch.Size([64, 3584])  
torch.Size([64, 3584])torch.Size([3584, 64])Adam-mini found the param block with name:

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weight base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight base_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weight  base_model.model.model.layers.27.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([18944, 64]) torch.Size([512, 64])  

base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])torch.Size([18944, 64]) 

torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name:  base_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weight base_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weight   base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weight torch.Size([512, 64])torch.Size([3584, 64])base_model.model.model.layers.27.mlp.down_proj.lora_a.default.weight 
torch.Size([64, 3584]) 
torch.Size([64, 18944])torch.Size([64, 18944])


Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name: Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.25.mlp.down_proj.lora_b.default.weight   base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weight  base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weighttorch.Size([3584, 64])torch.Size([512, 64]) base_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584]) 

torch.Size([3584, 64])
 
Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weight  torch.Size([64, 3584])torch.Size([512, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:   torch.Size([64, 3584])base_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weighttorch.Size([64, 3584])
 Adam-mini found the param block with name:
torch.Size([3584, 64])Adam-mini found the param block with name: 
 base_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weight base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])  torch.Size([3584, 64])
base_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weight
 torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weight  torch.Size([512, 64])torch.Size([64, 3584])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weight  Adam-mini found the param block with name:base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584])  
base_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:torch.Size([64, 3584])  base_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:
 torch.Size([18944, 64]) torch.Size([64, 3584])
base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:
  base_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weighttorch.Size([18944, 64]) 
torch.Size([512, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name: 
base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name: Adam-mini found the param block with name: torch.Size([64, 3584]) Adam-mini found the param block with name:base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weight
  base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weighttorch.Size([18944, 64])  Adam-mini found the param block with name:
torch.Size([64, 3584])torch.Size([18944, 64]) 
base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weight
 torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64]) 
base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:
 base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:
 base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weight torch.Size([64, 3584])base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weight 
Adam-mini found the param block with name: torch.Size([3584, 64]) 
torch.Size([64, 3584])base_model.model.model.layers.26.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: 
 torch.Size([3584, 64])base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weight
 Adam-mini found the param block with name:torch.Size([3584, 64]) 
base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weighttorch.Size([64, 3584])Adam-mini found the param block with name:
Adam-mini found the param block with name:   torch.Size([64, 3584])Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weight
   base_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weighttorch.Size([64, 18944])torch.Size([3584, 64]) 

torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([18944, 64])base_model.model.model.layers.12.mlp.down_proj.lora_b.default.weight
 torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weight  torch.Size([64, 3584])Adam-mini found the param block with name:torch.Size([64, 3584])
 
base_model.model.model.layers.26.mlp.up_proj.lora_a.default.weightAdam-mini found the param block with name:  Adam-mini found the param block with name:torch.Size([64, 3584])Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight
 base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weight base_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([512, 64])torch.Size([64, 3584]) torch.Size([512, 64])base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weight


 torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
Adam-mini found the param block with name:base_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weight  torch.Size([64, 3584]) base_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight
 torch.Size([64, 18944]) 
torch.Size([64, 3584])torch.Size([512, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  Adam-mini found the param block with name:base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weightbase_model.model.model.layers.26.mlp.down_proj.lora_b.default.weight  base_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([3584, 64])torch.Size([512, 64])torch.Size([512, 64]) 


base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weight
 torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:
  base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weight  Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weight torch.Size([64, 3584]) base_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weighttorch.Size([64, 3584])torch.Size([3584, 64]) 


torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weight  torch.Size([18944, 64])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:
   base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weightbase_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weight  Adam-mini found the param block with name: torch.Size([64, 3584])torch.Size([3584, 64]) 
torch.Size([18944, 64])base_model.model.model.layers.27.mlp.up_proj.lora_b.default.weight

 Adam-mini found the param block with name:torch.Size([18944, 64]) 
base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weight  base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])torch.Size([64, 3584])Adam-mini found the param block with name:

Adam-mini found the param block with name:  base_model.model.model.layers.27.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:Adam-mini found the param block with name: torch.Size([64, 18944])  
torch.Size([64, 3584])base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weightbase_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weight
 Adam-mini found the param block with name: torch.Size([18944, 64]) Adam-mini found the param block with name:
torch.Size([18944, 64])base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weight 
base_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight  torch.Size([3584, 64])torch.Size([512, 64])

Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.12.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight  torch.Size([3584, 64])base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584])
 
torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:base_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weight   torch.Size([64, 3584])base_model.model.model.layers.13.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584])
 
torch.Size([3584, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weight  base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.27.mlp.up_proj.lora_a.default.weight   torch.Size([64, 3584])torch.Size([64, 3584])
torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64])Adam-mini found the param block with name: 
 base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.27.mlp.up_proj.lora_b.default.weight  torch.Size([3584, 64])torch.Size([18944, 64])

Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:Adam-mini found the param block with name:   base_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weightbase_model.model.model.layers.27.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weight   torch.Size([64, 3584])torch.Size([512, 64])torch.Size([64, 18944])


Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weight  torch.Size([3584, 64])torch.Size([512, 64])Adam-mini found the param block with name:

 base_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weight  base_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weighttorch.Size([18944, 64]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64])
 base_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weight 
torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weight  base_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weightbase_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight  torch.Size([512, 64])torch.Size([3584, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight  base_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weight torch.Size([18944, 64])torch.Size([512, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.15.mlp.up_proj.lora_b.default.weight  base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weighttorch.Size([18944, 64]) 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.16.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weight  base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weight
 torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name: torch.Size([64, 3584])base_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weight base_model.model.model.layers.16.mlp.up_proj.lora_b.default.weight torch.Size([3584, 64])torch.Size([18944, 64])

Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:
 base_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64])
 base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weight  torch.Size([64, 18944])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weight base_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weight  torch.Size([3584, 64])
torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64]) 
base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3584, 64])base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weighttorch.Size([64, 3584]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weightbase_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weight  base_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weight base_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weight  torch.Size([18944, 64])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight  base_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 3584]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weight  base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weighttorch.Size([18944, 64])
 torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weight  base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weighttorch.Size([64, 3584]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])
 base_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64])
 base_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([3584, 64])base_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weight  base_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.20.mlp.down_proj.lora_a.default.weight  base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weight torch.Size([64, 18944])torch.Size([512, 64])

Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weightbase_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weight  torch.Size([3584, 64])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weight  base_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 18944]) 
base_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weight  base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weighttorch.Size([3584, 64]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([512, 64]) 
base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weightbase_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 18944])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weight  torch.Size([512, 64])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight  base_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weighttorch.Size([3584, 64]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weight 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.23.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:
 base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weightbase_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weightbase_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weight  torch.Size([512, 64])torch.Size([18944, 64])

Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.24.mlp.up_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weight torch.Size([512, 64])torch.Size([18944, 64])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weight  base_model.model.model.layers.24.mlp.down_proj.lora_a.default.weighttorch.Size([64, 3584]) 
torch.Size([64, 18944])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.24.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weight  torch.Size([3584, 64])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weightbase_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weight  torch.Size([64, 3584])torch.Size([18944, 64])

Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weight  base_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weight torch.Size([18944, 64])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.24.mlp.down_proj.lora_b.default.weightbase_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weightAdam-mini found the param block with name:  torch.Size([64, 3584])base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weight  base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weighttorch.Size([512, 64]) 
torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weightbase_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weight  torch.Size([64, 3584])
torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight  base_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weighttorch.Size([18944, 64]) 
torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weight  base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weight torch.Size([64, 18944])torch.Size([64, 3584])

Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.25.mlp.down_proj.lora_b.default.weight  base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])Adam-mini found the param block with name:
 base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weight  base_model.model.model.layers.25.mlp.down_proj.lora_b.default.weight torch.Size([64, 3584])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])Adam-mini found the param block with name:
 base_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weight base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weighttorch.Size([512, 64]) 
torch.Size([18944, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.26.mlp.up_proj.lora_a.default.weight  torch.Size([64, 3584])torch.Size([64, 3584])

Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weighttorch.Size([18944, 64]) 
torch.Size([512, 64])
Adam-mini found the param block with name:Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weight  base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weight torch.Size([64, 18944])
torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.26.mlp.down_proj.lora_b.default.weight  base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weighttorch.Size([3584, 64]) 
torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584]) 
base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: Adam-mini found the param block with name:base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weight  base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weighttorch.Size([64, 3584]) 
torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name:Adam-mini found the param block with name:  base_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weightbase_model.model.model.layers.26.mlp.down_proj.lora_b.default.weight  torch.Size([64, 3584])torch.Size([3584, 64])

Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weight Adam-mini found the param block with name: torch.Size([3584, 64])base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weight
 torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weightAdam-mini found the param block with name:  base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weighttorch.Size([64, 3584]) 
torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weightAdam-mini found the param block with name:  torch.Size([18944, 64])base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight
 torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_a.default.weight Adam-mini found the param block with name:torch.Size([64, 3584])
 base_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_b.default.weight Adam-mini found the param block with name:torch.Size([18944, 64]) base_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight
 torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])Adam-mini found the param block with name:
 base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.0.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.1.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.2.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.3.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.4.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.5.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.6.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.7.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.8.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.9.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.10.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.11.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.12.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.13.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.14.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.15.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.16.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.17.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.18.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.19.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.20.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.21.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.22.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.23.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.24.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.25.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.26.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.27.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.28.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.29.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.30.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.qkv.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.qkv.lora_b.default.weight torch.Size([3840, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.attn.proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.gate_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.gate_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.up_proj.lora_a.default.weight torch.Size([64, 1280])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.up_proj.lora_b.default.weight torch.Size([3420, 64])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.down_proj.lora_a.default.weight torch.Size([64, 3420])
Adam-mini found the param block with name: base_model.model.visual.blocks.31.mlp.down_proj.lora_b.default.weight torch.Size([1280, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.0.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.1.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.2.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.3.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.4.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.5.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.6.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.7.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.8.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.9.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.10.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.11.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.12.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.13.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.14.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.15.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.16.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.17.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.18.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.19.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.20.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.21.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.22.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.23.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.24.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.25.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.26.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.q_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.k_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.v_proj.lora_b.default.weight torch.Size([512, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.self_attn.o_proj.lora_b.default.weight torch.Size([3584, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.gate_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_a.default.weight torch.Size([64, 3584])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.up_proj.lora_b.default.weight torch.Size([18944, 64])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_a.default.weight torch.Size([64, 18944])
Adam-mini found the param block with name: base_model.model.model.layers.27.mlp.down_proj.lora_b.default.weight torch.Size([3584, 64])
[INFO|2025-04-06 12:46:52] llamafactory.train.trainer_utils:157 >> Using Adam-mini optimizer.
[2025-04-06 12:46:52,976] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-04-06 12:46:52,976] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-04-06 12:46:53,026] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-06 12:46:53,031] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-04-06 12:46:53,032] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-06 12:46:53,269] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam_mini
[2025-04-06 12:46:53,269] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam_mini type=<class 'adam_mini.adam_mini.Adam_mini'>
[2025-04-06 12:46:53,269] [WARNING] [engine.py:1315:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2025-04-06 12:46:53,270] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-04-06 12:46:53,270] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-04-06 12:46:53,488] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-04-06 12:46:53,489] [INFO] [utils.py:782:see_memory_usage] MA 2.33 GB         Max_MA 4.86 GB         CA 3.86 GB         Max_CA 6 GB 
[2025-04-06 12:46:53,489] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.44 GB, percent = 3.5%
[2025-04-06 12:46:53,504] [INFO] [stage3.py:170:__init__] Reduce bucket size 12845056
[2025-04-06 12:46:53,504] [INFO] [stage3.py:171:__init__] Prefetch bucket size 11560550
[2025-04-06 12:46:53,708] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-04-06 12:46:53,709] [INFO] [utils.py:782:see_memory_usage] MA 2.33 GB         Max_MA 2.33 GB         CA 3.86 GB         Max_CA 4 GB 
[2025-04-06 12:46:53,709] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.45 GB, percent = 3.5%
Parameter Offload: Total persistent parameters: 2683904 in 424 params
[2025-04-06 12:46:54,424] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-04-06 12:46:54,424] [INFO] [utils.py:782:see_memory_usage] MA 1.99 GB         Max_MA 2.33 GB         CA 3.86 GB         Max_CA 4 GB 
[2025-04-06 12:46:54,425] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.47 GB, percent = 3.5%
[2025-04-06 12:46:54,631] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-04-06 12:46:54,632] [INFO] [utils.py:782:see_memory_usage] MA 1.99 GB         Max_MA 1.99 GB         CA 3.86 GB         Max_CA 4 GB 
[2025-04-06 12:46:54,632] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.46 GB, percent = 3.5%
[2025-04-06 12:46:56,362] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 712
[2025-04-06 12:46:56,363] [INFO] [utils.py:782:see_memory_usage] MA 1.99 GB         Max_MA 1.99 GB         CA 3.44 GB         Max_CA 4 GB 
[2025-04-06 12:46:56,363] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.76 GB, percent = 3.5%
[2025-04-06 12:46:56,599] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-04-06 12:46:56,599] [INFO] [utils.py:782:see_memory_usage] MA 1.99 GB         Max_MA 1.99 GB         CA 3.44 GB         Max_CA 3 GB 
[2025-04-06 12:46:56,600] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.76 GB, percent = 3.5%
[2025-04-06 12:46:57,544] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-04-06 12:46:57,545] [INFO] [utils.py:782:see_memory_usage] MA 2.09 GB         Max_MA 2.09 GB         CA 3.53 GB         Max_CA 4 GB 
[2025-04-06 12:46:57,545] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.76 GB, percent = 3.5%
[2025-04-06 12:46:57,772] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-06 12:46:57,773] [INFO] [utils.py:782:see_memory_usage] MA 2.09 GB         Max_MA 2.09 GB         CA 3.53 GB         Max_CA 4 GB 
[2025-04-06 12:46:57,773] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.76 GB, percent = 3.5%
[2025-04-06 12:46:58,002] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-06 12:46:58,003] [INFO] [utils.py:782:see_memory_usage] MA 2.09 GB         Max_MA 2.09 GB         CA 3.53 GB         Max_CA 4 GB 
[2025-04-06 12:46:58,003] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.76 GB, percent = 3.5%
[2025-04-06 12:46:58,004] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
[2025-04-06 12:46:58,496] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-06 12:46:58,497] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 3.53 GB         Max_CA 4 GB 
[2025-04-06 12:46:58,497] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.81 GB, percent = 3.6%
[2025-04-06 12:46:58,498] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-04-06 12:46:58,498] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-06 12:46:58,498] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-06 12:46:58,498] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], mom=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-04-06 12:46:58,508] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-04-06 12:46:58,508] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-06 12:46:58,508] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-06 12:46:58,508] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-04-06 12:46:58,508] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f46d3e36020>
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-06 12:46:58,509] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 8
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-04-06 12:46:58,510] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   optimizer_name ............... None
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   optimizer_params ............. None
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   scheduler_name ............... None
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   scheduler_params ............. None
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-04-06 12:46:58,511] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   train_batch_size ............. 64
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  1
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   world_size ................... 8
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  True
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=12845056 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=11560550 param_persistence_threshold=35840 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-06 12:46:58,512] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 3
[2025-04-06 12:46:58,512] [INFO] [config.py:991:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.284506e+07, 
        "stage3_prefetch_bucket_size": 1.156055e+07, 
        "stage3_param_persistence_threshold": 3.584000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2405] 2025-04-06 12:46:58,514 >> ***** Running training *****
[INFO|trainer.py:2406] 2025-04-06 12:46:58,514 >>   Num examples = 94,162
[INFO|trainer.py:2407] 2025-04-06 12:46:58,514 >>   Num Epochs = 10
[INFO|trainer.py:2408] 2025-04-06 12:46:58,514 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2411] 2025-04-06 12:46:58,514 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2412] 2025-04-06 12:46:58,514 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2413] 2025-04-06 12:46:58,514 >>   Total optimization steps = 14,710
[INFO|trainer.py:2414] 2025-04-06 12:46:58,520 >>   Number of trainable parameters = 206,086,144

  0%|          | 0/14710 [00:00<?, ?it/s][rank5]: Traceback (most recent call last):
[rank5]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank5]:     launch()
[rank5]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank5]:     run_exp()
[rank5]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank5]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank5]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank5]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank5]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank5]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank5]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank5]:     outputs = model(**inputs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank5]:     loss = self.module(*inputs, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank5]:     return inner()
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank5]:     return self.base_model(
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank5]:     return inner()
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank5]:     return self.model.forward(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank5]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank5]:     return inner()
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank5]:     hidden_states = self._gradient_checkpointing_func(
[rank5]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank5]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank5]:     return CheckpointFunction.apply(function, preserve, *args)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank5]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank5]:     outputs = run_function(*args)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank5]:     return inner()
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank5]:     hidden_states = hidden_states + self.attn(
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank5]:     return inner()
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank5]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank5]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank5]:     return ApplyRotaryEmb.apply(
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank5]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank5]:     out = apply_rotary(
[rank5]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank5]:     x.dtype == cos.dtype
[rank5]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16
[rank2]: Traceback (most recent call last):
[rank2]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank2]:     launch()
[rank2]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank2]:     run_exp()
[rank2]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank2]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank2]:     return self.base_model(
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank2]:     return self.model.forward(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank2]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank2]:     hidden_states = self._gradient_checkpointing_func(
[rank2]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank2]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank2]:     return CheckpointFunction.apply(function, preserve, *args)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank2]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank2]:     outputs = run_function(*args)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank2]:     hidden_states = hidden_states + self.attn(
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank2]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank2]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank2]:     return ApplyRotaryEmb.apply(
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank2]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank2]:     out = apply_rotary(
[rank2]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank2]:     x.dtype == cos.dtype
[rank2]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16
[rank7]: Traceback (most recent call last):
[rank7]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank7]:     launch()
[rank7]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank7]:     run_exp()
[rank7]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank7]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank7]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank7]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank7]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank7]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank7]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank7]:     outputs = model(**inputs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank7]:     loss = self.module(*inputs, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank7]:     return inner()
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank7]:     return self.base_model(
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank7]:     return inner()
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank7]:     return self.model.forward(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank7]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank7]:     return inner()
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank7]:     hidden_states = self._gradient_checkpointing_func(
[rank7]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank7]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank7]:     return CheckpointFunction.apply(function, preserve, *args)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank7]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank7]:     outputs = run_function(*args)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank7]:     return inner()
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank7]:     hidden_states = hidden_states + self.attn(
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank7]:     return inner()
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank7]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank7]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank7]:     return ApplyRotaryEmb.apply(
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank7]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank7]:     out = apply_rotary(
[rank7]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank7]:     x.dtype == cos.dtype
[rank7]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16
[rank3]: Traceback (most recent call last):
[rank3]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank3]:     launch()
[rank3]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank3]:     run_exp()
[rank3]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank3]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank3]:     return self.base_model(
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank3]:     return self.model.forward(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank3]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank3]:     hidden_states = self._gradient_checkpointing_func(
[rank3]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank3]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank3]:     return CheckpointFunction.apply(function, preserve, *args)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank3]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank3]:     outputs = run_function(*args)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank3]:     hidden_states = hidden_states + self.attn(
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank3]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank3]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank3]:     return ApplyRotaryEmb.apply(
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank3]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank3]:     out = apply_rotary(
[rank3]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank3]:     x.dtype == cos.dtype
[rank3]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16
[rank6]: Traceback (most recent call last):
[rank6]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank6]:     launch()
[rank6]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank6]:     run_exp()
[rank6]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank6]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank6]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank6]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank6]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank6]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank6]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank6]:     outputs = model(**inputs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank6]:     loss = self.module(*inputs, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank6]:     return inner()
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank6]:     return self.base_model(
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank6]:     return inner()
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank6]:     return self.model.forward(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank6]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank6]:     return inner()
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank6]:     hidden_states = self._gradient_checkpointing_func(
[rank6]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank6]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank6]:     return CheckpointFunction.apply(function, preserve, *args)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank6]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank6]:     outputs = run_function(*args)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank6]:     return inner()
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank6]:     hidden_states = hidden_states + self.attn(
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank6]:     return inner()
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank6]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank6]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank6]:     return ApplyRotaryEmb.apply(
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank6]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank6]:     out = apply_rotary(
[rank6]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank6]:     x.dtype == cos.dtype
[rank6]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank0]:     launch()
[rank0]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank0]:     run_exp()
[rank0]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank0]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank0]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank0]:     hidden_states = self._gradient_checkpointing_func(
[rank0]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank0]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank0]:     return CheckpointFunction.apply(function, preserve, *args)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank0]:     outputs = run_function(*args)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank0]:     hidden_states = hidden_states + self.attn(
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank0]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank0]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank0]:     return ApplyRotaryEmb.apply(
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank0]:     out = apply_rotary(
[rank0]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank0]:     x.dtype == cos.dtype
[rank0]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16
[rank1]: Traceback (most recent call last):
[rank1]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank1]:     launch()
[rank1]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank1]:     run_exp()
[rank1]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank1]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank1]:     return self.base_model(
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank1]:     return self.model.forward(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank1]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank1]:     hidden_states = self._gradient_checkpointing_func(
[rank1]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank1]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank1]:     return CheckpointFunction.apply(function, preserve, *args)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank1]:     outputs = run_function(*args)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank1]:     hidden_states = hidden_states + self.attn(
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank1]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank1]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank1]:     return ApplyRotaryEmb.apply(
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank1]:     out = apply_rotary(
[rank1]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank1]:     x.dtype == cos.dtype
[rank1]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16
[rank4]: Traceback (most recent call last):
[rank4]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank4]:     launch()
[rank4]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank4]:     run_exp()
[rank4]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 93, in run_exp
[rank4]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank4]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/tuner.py", line 67, in _training_function
[rank4]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank4]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank4]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2241, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3698, in training_step
[rank4]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/trainer.py", line 3759, in compute_loss
[rank4]:     outputs = model(**inputs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1987, in forward
[rank4]:     loss = self.module(*inputs, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank4]:     return inner()
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/peft_model.py", line 1577, in forward
[rank4]:     return self.base_model(
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank4]:     return inner()
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank4]:     return self.model.forward(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1813, in forward
[rank4]:     video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank4]:     return inner()
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 554, in forward
[rank4]:     hidden_states = self._gradient_checkpointing_func(
[rank4]:   File "/data3/cmitra/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank4]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank4]:     return CheckpointFunction.apply(function, preserve, *args)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank4]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank4]:     outputs = run_function(*args)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank4]:     return inner()
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 351, in forward
[rank4]:     hidden_states = hidden_states + self.attn(
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank4]:     return inner()
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 201, in forward
[rank4]:     q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 168, in apply_rotary_pos_emb_flashatt
[rank4]:     q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 122, in apply_rotary_emb
[rank4]:     return ApplyRotaryEmb.apply(
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank4]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/layers/rotary.py", line 48, in forward
[rank4]:     out = apply_rotary(
[rank4]:   File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/flash_attn/ops/triton/rotary.py", line 176, in apply_rotary
[rank4]:     x.dtype == cos.dtype
[rank4]: AssertionError: Input and cos/sin must have the same dtype, got torch.float32 and torch.bfloat16

  0%|          | 0/14710 [00:54<?, ?it/s]
W0406 12:47:54.690000 241828 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 241906 closing signal SIGTERM
W0406 12:47:54.691000 241828 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 241909 closing signal SIGTERM
W0406 12:47:54.692000 241828 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 241910 closing signal SIGTERM
W0406 12:47:54.692000 241828 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 241911 closing signal SIGTERM
W0406 12:47:54.692000 241828 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 241912 closing signal SIGTERM
E0406 12:47:55.271000 241828 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 241907) of binary: /home/cmitra/anaconda3/envs/lfact/bin/python
Traceback (most recent call last):
  File "/home/cmitra/anaconda3/envs/lfact/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/cmitra/anaconda3/envs/lfact/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data3/cmitra/LLaMA-Factory/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-04-06_12:47:54
  host      : trinity-2-29.eth
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 241908)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-04-06_12:47:54
  host      : trinity-2-29.eth
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 241913)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-06_12:47:54
  host      : trinity-2-29.eth
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 241907)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
